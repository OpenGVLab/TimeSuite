{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client on!\n",
      "\n",
      "model_type               : VideoChat2_it4_mistral_LinearProAda\n",
      "model_dir                : ./download/parameters\n",
      "model_pth                : timesuite\n",
      "output_dir               : Please input model output dir!\n",
      "batch_size               : 1\n",
      "infer_clip_frames        : 8\n",
      "vision_encoder.num_frames: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e245f76456254890ad814feb9d22a1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable good initialization!\n",
      "_IncompatibleKeys(missing_keys=['vision_encoder.encoder.patch_embed.proj.weight', 'vision_encoder.encoder.patch_embed.proj.bias', 'vision_encoder.encoder.blocks.0.norm1.weight', 'vision_encoder.encoder.blocks.0.norm1.bias', 'vision_encoder.encoder.blocks.0.attn.q_bias', 'vision_encoder.encoder.blocks.0.attn.v_bias', 'vision_encoder.encoder.blocks.0.attn.qkv.weight', 'vision_encoder.encoder.blocks.0.attn.proj.weight', 'vision_encoder.encoder.blocks.0.attn.proj.bias', 'vision_encoder.encoder.blocks.0.norm2.weight', 'vision_encoder.encoder.blocks.0.norm2.bias', 'vision_encoder.encoder.blocks.0.mlp.fc1.weight', 'vision_encoder.encoder.blocks.0.mlp.fc1.bias', 'vision_encoder.encoder.blocks.0.mlp.fc2.weight', 'vision_encoder.encoder.blocks.0.mlp.fc2.bias', 'vision_encoder.encoder.blocks.1.norm1.weight', 'vision_encoder.encoder.blocks.1.norm1.bias', 'vision_encoder.encoder.blocks.1.attn.q_bias', 'vision_encoder.encoder.blocks.1.attn.v_bias', 'vision_encoder.encoder.blocks.1.attn.qkv.weight', 'vision_encoder.encoder.blocks.1.attn.proj.weight', 'vision_encoder.encoder.blocks.1.attn.proj.bias', 'vision_encoder.encoder.blocks.1.norm2.weight', 'vision_encoder.encoder.blocks.1.norm2.bias', 'vision_encoder.encoder.blocks.1.mlp.fc1.weight', 'vision_encoder.encoder.blocks.1.mlp.fc1.bias', 'vision_encoder.encoder.blocks.1.mlp.fc2.weight', 'vision_encoder.encoder.blocks.1.mlp.fc2.bias', 'vision_encoder.encoder.blocks.2.norm1.weight', 'vision_encoder.encoder.blocks.2.norm1.bias', 'vision_encoder.encoder.blocks.2.attn.q_bias', 'vision_encoder.encoder.blocks.2.attn.v_bias', 'vision_encoder.encoder.blocks.2.attn.qkv.weight', 'vision_encoder.encoder.blocks.2.attn.proj.weight', 'vision_encoder.encoder.blocks.2.attn.proj.bias', 'vision_encoder.encoder.blocks.2.norm2.weight', 'vision_encoder.encoder.blocks.2.norm2.bias', 'vision_encoder.encoder.blocks.2.mlp.fc1.weight', 'vision_encoder.encoder.blocks.2.mlp.fc1.bias', 'vision_encoder.encoder.blocks.2.mlp.fc2.weight', 'vision_encoder.encoder.blocks.2.mlp.fc2.bias', 'vision_encoder.encoder.blocks.3.norm1.weight', 'vision_encoder.encoder.blocks.3.norm1.bias', 'vision_encoder.encoder.blocks.3.attn.q_bias', 'vision_encoder.encoder.blocks.3.attn.v_bias', 'vision_encoder.encoder.blocks.3.attn.qkv.weight', 'vision_encoder.encoder.blocks.3.attn.proj.weight', 'vision_encoder.encoder.blocks.3.attn.proj.bias', 'vision_encoder.encoder.blocks.3.norm2.weight', 'vision_encoder.encoder.blocks.3.norm2.bias', 'vision_encoder.encoder.blocks.3.mlp.fc1.weight', 'vision_encoder.encoder.blocks.3.mlp.fc1.bias', 'vision_encoder.encoder.blocks.3.mlp.fc2.weight', 'vision_encoder.encoder.blocks.3.mlp.fc2.bias', 'vision_encoder.encoder.blocks.4.norm1.weight', 'vision_encoder.encoder.blocks.4.norm1.bias', 'vision_encoder.encoder.blocks.4.attn.q_bias', 'vision_encoder.encoder.blocks.4.attn.v_bias', 'vision_encoder.encoder.blocks.4.attn.qkv.weight', 'vision_encoder.encoder.blocks.4.attn.proj.weight', 'vision_encoder.encoder.blocks.4.attn.proj.bias', 'vision_encoder.encoder.blocks.4.norm2.weight', 'vision_encoder.encoder.blocks.4.norm2.bias', 'vision_encoder.encoder.blocks.4.mlp.fc1.weight', 'vision_encoder.encoder.blocks.4.mlp.fc1.bias', 'vision_encoder.encoder.blocks.4.mlp.fc2.weight', 'vision_encoder.encoder.blocks.4.mlp.fc2.bias', 'vision_encoder.encoder.blocks.5.norm1.weight', 'vision_encoder.encoder.blocks.5.norm1.bias', 'vision_encoder.encoder.blocks.5.attn.q_bias', 'vision_encoder.encoder.blocks.5.attn.v_bias', 'vision_encoder.encoder.blocks.5.attn.qkv.weight', 'vision_encoder.encoder.blocks.5.attn.proj.weight', 'vision_encoder.encoder.blocks.5.attn.proj.bias', 'vision_encoder.encoder.blocks.5.norm2.weight', 'vision_encoder.encoder.blocks.5.norm2.bias', 'vision_encoder.encoder.blocks.5.mlp.fc1.weight', 'vision_encoder.encoder.blocks.5.mlp.fc1.bias', 'vision_encoder.encoder.blocks.5.mlp.fc2.weight', 'vision_encoder.encoder.blocks.5.mlp.fc2.bias', 'vision_encoder.encoder.blocks.6.norm1.weight', 'vision_encoder.encoder.blocks.6.norm1.bias', 'vision_encoder.encoder.blocks.6.attn.q_bias', 'vision_encoder.encoder.blocks.6.attn.v_bias', 'vision_encoder.encoder.blocks.6.attn.qkv.weight', 'vision_encoder.encoder.blocks.6.attn.proj.weight', 'vision_encoder.encoder.blocks.6.attn.proj.bias', 'vision_encoder.encoder.blocks.6.norm2.weight', 'vision_encoder.encoder.blocks.6.norm2.bias', 'vision_encoder.encoder.blocks.6.mlp.fc1.weight', 'vision_encoder.encoder.blocks.6.mlp.fc1.bias', 'vision_encoder.encoder.blocks.6.mlp.fc2.weight', 'vision_encoder.encoder.blocks.6.mlp.fc2.bias', 'vision_encoder.encoder.blocks.7.norm1.weight', 'vision_encoder.encoder.blocks.7.norm1.bias', 'vision_encoder.encoder.blocks.7.attn.q_bias', 'vision_encoder.encoder.blocks.7.attn.v_bias', 'vision_encoder.encoder.blocks.7.attn.qkv.weight', 'vision_encoder.encoder.blocks.7.attn.proj.weight', 'vision_encoder.encoder.blocks.7.attn.proj.bias', 'vision_encoder.encoder.blocks.7.norm2.weight', 'vision_encoder.encoder.blocks.7.norm2.bias', 'vision_encoder.encoder.blocks.7.mlp.fc1.weight', 'vision_encoder.encoder.blocks.7.mlp.fc1.bias', 'vision_encoder.encoder.blocks.7.mlp.fc2.weight', 'vision_encoder.encoder.blocks.7.mlp.fc2.bias', 'vision_encoder.encoder.blocks.8.norm1.weight', 'vision_encoder.encoder.blocks.8.norm1.bias', 'vision_encoder.encoder.blocks.8.attn.q_bias', 'vision_encoder.encoder.blocks.8.attn.v_bias', 'vision_encoder.encoder.blocks.8.attn.qkv.weight', 'vision_encoder.encoder.blocks.8.attn.proj.weight', 'vision_encoder.encoder.blocks.8.attn.proj.bias', 'vision_encoder.encoder.blocks.8.norm2.weight', 'vision_encoder.encoder.blocks.8.norm2.bias', 'vision_encoder.encoder.blocks.8.mlp.fc1.weight', 'vision_encoder.encoder.blocks.8.mlp.fc1.bias', 'vision_encoder.encoder.blocks.8.mlp.fc2.weight', 'vision_encoder.encoder.blocks.8.mlp.fc2.bias', 'vision_encoder.encoder.blocks.9.norm1.weight', 'vision_encoder.encoder.blocks.9.norm1.bias', 'vision_encoder.encoder.blocks.9.attn.q_bias', 'vision_encoder.encoder.blocks.9.attn.v_bias', 'vision_encoder.encoder.blocks.9.attn.qkv.weight', 'vision_encoder.encoder.blocks.9.attn.proj.weight', 'vision_encoder.encoder.blocks.9.attn.proj.bias', 'vision_encoder.encoder.blocks.9.norm2.weight', 'vision_encoder.encoder.blocks.9.norm2.bias', 'vision_encoder.encoder.blocks.9.mlp.fc1.weight', 'vision_encoder.encoder.blocks.9.mlp.fc1.bias', 'vision_encoder.encoder.blocks.9.mlp.fc2.weight', 'vision_encoder.encoder.blocks.9.mlp.fc2.bias', 'vision_encoder.encoder.blocks.10.norm1.weight', 'vision_encoder.encoder.blocks.10.norm1.bias', 'vision_encoder.encoder.blocks.10.attn.q_bias', 'vision_encoder.encoder.blocks.10.attn.v_bias', 'vision_encoder.encoder.blocks.10.attn.qkv.weight', 'vision_encoder.encoder.blocks.10.attn.proj.weight', 'vision_encoder.encoder.blocks.10.attn.proj.bias', 'vision_encoder.encoder.blocks.10.norm2.weight', 'vision_encoder.encoder.blocks.10.norm2.bias', 'vision_encoder.encoder.blocks.10.mlp.fc1.weight', 'vision_encoder.encoder.blocks.10.mlp.fc1.bias', 'vision_encoder.encoder.blocks.10.mlp.fc2.weight', 'vision_encoder.encoder.blocks.10.mlp.fc2.bias', 'vision_encoder.encoder.blocks.11.norm1.weight', 'vision_encoder.encoder.blocks.11.norm1.bias', 'vision_encoder.encoder.blocks.11.attn.q_bias', 'vision_encoder.encoder.blocks.11.attn.v_bias', 'vision_encoder.encoder.blocks.11.attn.qkv.weight', 'vision_encoder.encoder.blocks.11.attn.proj.weight', 'vision_encoder.encoder.blocks.11.attn.proj.bias', 'vision_encoder.encoder.blocks.11.norm2.weight', 'vision_encoder.encoder.blocks.11.norm2.bias', 'vision_encoder.encoder.blocks.11.mlp.fc1.weight', 'vision_encoder.encoder.blocks.11.mlp.fc1.bias', 'vision_encoder.encoder.blocks.11.mlp.fc2.weight', 'vision_encoder.encoder.blocks.11.mlp.fc2.bias', 'vision_encoder.encoder.blocks.12.norm1.weight', 'vision_encoder.encoder.blocks.12.norm1.bias', 'vision_encoder.encoder.blocks.12.attn.q_bias', 'vision_encoder.encoder.blocks.12.attn.v_bias', 'vision_encoder.encoder.blocks.12.attn.qkv.weight', 'vision_encoder.encoder.blocks.12.attn.proj.weight', 'vision_encoder.encoder.blocks.12.attn.proj.bias', 'vision_encoder.encoder.blocks.12.norm2.weight', 'vision_encoder.encoder.blocks.12.norm2.bias', 'vision_encoder.encoder.blocks.12.mlp.fc1.weight', 'vision_encoder.encoder.blocks.12.mlp.fc1.bias', 'vision_encoder.encoder.blocks.12.mlp.fc2.weight', 'vision_encoder.encoder.blocks.12.mlp.fc2.bias', 'vision_encoder.encoder.blocks.13.norm1.weight', 'vision_encoder.encoder.blocks.13.norm1.bias', 'vision_encoder.encoder.blocks.13.attn.q_bias', 'vision_encoder.encoder.blocks.13.attn.v_bias', 'vision_encoder.encoder.blocks.13.attn.qkv.weight', 'vision_encoder.encoder.blocks.13.attn.proj.weight', 'vision_encoder.encoder.blocks.13.attn.proj.bias', 'vision_encoder.encoder.blocks.13.norm2.weight', 'vision_encoder.encoder.blocks.13.norm2.bias', 'vision_encoder.encoder.blocks.13.mlp.fc1.weight', 'vision_encoder.encoder.blocks.13.mlp.fc1.bias', 'vision_encoder.encoder.blocks.13.mlp.fc2.weight', 'vision_encoder.encoder.blocks.13.mlp.fc2.bias', 'vision_encoder.encoder.blocks.14.norm1.weight', 'vision_encoder.encoder.blocks.14.norm1.bias', 'vision_encoder.encoder.blocks.14.attn.q_bias', 'vision_encoder.encoder.blocks.14.attn.v_bias', 'vision_encoder.encoder.blocks.14.attn.qkv.weight', 'vision_encoder.encoder.blocks.14.attn.proj.weight', 'vision_encoder.encoder.blocks.14.attn.proj.bias', 'vision_encoder.encoder.blocks.14.norm2.weight', 'vision_encoder.encoder.blocks.14.norm2.bias', 'vision_encoder.encoder.blocks.14.mlp.fc1.weight', 'vision_encoder.encoder.blocks.14.mlp.fc1.bias', 'vision_encoder.encoder.blocks.14.mlp.fc2.weight', 'vision_encoder.encoder.blocks.14.mlp.fc2.bias', 'vision_encoder.encoder.blocks.15.norm1.weight', 'vision_encoder.encoder.blocks.15.norm1.bias', 'vision_encoder.encoder.blocks.15.attn.q_bias', 'vision_encoder.encoder.blocks.15.attn.v_bias', 'vision_encoder.encoder.blocks.15.attn.qkv.weight', 'vision_encoder.encoder.blocks.15.attn.proj.weight', 'vision_encoder.encoder.blocks.15.attn.proj.bias', 'vision_encoder.encoder.blocks.15.norm2.weight', 'vision_encoder.encoder.blocks.15.norm2.bias', 'vision_encoder.encoder.blocks.15.mlp.fc1.weight', 'vision_encoder.encoder.blocks.15.mlp.fc1.bias', 'vision_encoder.encoder.blocks.15.mlp.fc2.weight', 'vision_encoder.encoder.blocks.15.mlp.fc2.bias', 'vision_encoder.encoder.blocks.16.norm1.weight', 'vision_encoder.encoder.blocks.16.norm1.bias', 'vision_encoder.encoder.blocks.16.attn.q_bias', 'vision_encoder.encoder.blocks.16.attn.v_bias', 'vision_encoder.encoder.blocks.16.attn.qkv.weight', 'vision_encoder.encoder.blocks.16.attn.proj.weight', 'vision_encoder.encoder.blocks.16.attn.proj.bias', 'vision_encoder.encoder.blocks.16.norm2.weight', 'vision_encoder.encoder.blocks.16.norm2.bias', 'vision_encoder.encoder.blocks.16.mlp.fc1.weight', 'vision_encoder.encoder.blocks.16.mlp.fc1.bias', 'vision_encoder.encoder.blocks.16.mlp.fc2.weight', 'vision_encoder.encoder.blocks.16.mlp.fc2.bias', 'vision_encoder.encoder.blocks.17.norm1.weight', 'vision_encoder.encoder.blocks.17.norm1.bias', 'vision_encoder.encoder.blocks.17.attn.q_bias', 'vision_encoder.encoder.blocks.17.attn.v_bias', 'vision_encoder.encoder.blocks.17.attn.qkv.weight', 'vision_encoder.encoder.blocks.17.attn.proj.weight', 'vision_encoder.encoder.blocks.17.attn.proj.bias', 'vision_encoder.encoder.blocks.17.norm2.weight', 'vision_encoder.encoder.blocks.17.norm2.bias', 'vision_encoder.encoder.blocks.17.mlp.fc1.weight', 'vision_encoder.encoder.blocks.17.mlp.fc1.bias', 'vision_encoder.encoder.blocks.17.mlp.fc2.weight', 'vision_encoder.encoder.blocks.17.mlp.fc2.bias', 'vision_encoder.encoder.blocks.18.norm1.weight', 'vision_encoder.encoder.blocks.18.norm1.bias', 'vision_encoder.encoder.blocks.18.attn.q_bias', 'vision_encoder.encoder.blocks.18.attn.v_bias', 'vision_encoder.encoder.blocks.18.attn.qkv.weight', 'vision_encoder.encoder.blocks.18.attn.proj.weight', 'vision_encoder.encoder.blocks.18.attn.proj.bias', 'vision_encoder.encoder.blocks.18.norm2.weight', 'vision_encoder.encoder.blocks.18.norm2.bias', 'vision_encoder.encoder.blocks.18.mlp.fc1.weight', 'vision_encoder.encoder.blocks.18.mlp.fc1.bias', 'vision_encoder.encoder.blocks.18.mlp.fc2.weight', 'vision_encoder.encoder.blocks.18.mlp.fc2.bias', 'vision_encoder.encoder.blocks.19.norm1.weight', 'vision_encoder.encoder.blocks.19.norm1.bias', 'vision_encoder.encoder.blocks.19.attn.q_bias', 'vision_encoder.encoder.blocks.19.attn.v_bias', 'vision_encoder.encoder.blocks.19.attn.qkv.weight', 'vision_encoder.encoder.blocks.19.attn.proj.weight', 'vision_encoder.encoder.blocks.19.attn.proj.bias', 'vision_encoder.encoder.blocks.19.norm2.weight', 'vision_encoder.encoder.blocks.19.norm2.bias', 'vision_encoder.encoder.blocks.19.mlp.fc1.weight', 'vision_encoder.encoder.blocks.19.mlp.fc1.bias', 'vision_encoder.encoder.blocks.19.mlp.fc2.weight', 'vision_encoder.encoder.blocks.19.mlp.fc2.bias', 'vision_encoder.encoder.blocks.20.norm1.weight', 'vision_encoder.encoder.blocks.20.norm1.bias', 'vision_encoder.encoder.blocks.20.attn.q_bias', 'vision_encoder.encoder.blocks.20.attn.v_bias', 'vision_encoder.encoder.blocks.20.attn.qkv.weight', 'vision_encoder.encoder.blocks.20.attn.proj.weight', 'vision_encoder.encoder.blocks.20.attn.proj.bias', 'vision_encoder.encoder.blocks.20.norm2.weight', 'vision_encoder.encoder.blocks.20.norm2.bias', 'vision_encoder.encoder.blocks.20.mlp.fc1.weight', 'vision_encoder.encoder.blocks.20.mlp.fc1.bias', 'vision_encoder.encoder.blocks.20.mlp.fc2.weight', 'vision_encoder.encoder.blocks.20.mlp.fc2.bias', 'vision_encoder.encoder.blocks.21.norm1.weight', 'vision_encoder.encoder.blocks.21.norm1.bias', 'vision_encoder.encoder.blocks.21.attn.q_bias', 'vision_encoder.encoder.blocks.21.attn.v_bias', 'vision_encoder.encoder.blocks.21.attn.qkv.weight', 'vision_encoder.encoder.blocks.21.attn.proj.weight', 'vision_encoder.encoder.blocks.21.attn.proj.bias', 'vision_encoder.encoder.blocks.21.norm2.weight', 'vision_encoder.encoder.blocks.21.norm2.bias', 'vision_encoder.encoder.blocks.21.mlp.fc1.weight', 'vision_encoder.encoder.blocks.21.mlp.fc1.bias', 'vision_encoder.encoder.blocks.21.mlp.fc2.weight', 'vision_encoder.encoder.blocks.21.mlp.fc2.bias', 'vision_encoder.encoder.blocks.22.norm1.weight', 'vision_encoder.encoder.blocks.22.norm1.bias', 'vision_encoder.encoder.blocks.22.attn.q_bias', 'vision_encoder.encoder.blocks.22.attn.v_bias', 'vision_encoder.encoder.blocks.22.attn.qkv.weight', 'vision_encoder.encoder.blocks.22.attn.proj.weight', 'vision_encoder.encoder.blocks.22.attn.proj.bias', 'vision_encoder.encoder.blocks.22.norm2.weight', 'vision_encoder.encoder.blocks.22.norm2.bias', 'vision_encoder.encoder.blocks.22.mlp.fc1.weight', 'vision_encoder.encoder.blocks.22.mlp.fc1.bias', 'vision_encoder.encoder.blocks.22.mlp.fc2.weight', 'vision_encoder.encoder.blocks.22.mlp.fc2.bias', 'vision_layernorm.weight', 'vision_layernorm.bias', 'mistral_model.base_model.model.model.embed_tokens.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.0.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.1.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.2.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.3.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.4.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.5.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.6.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.7.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.8.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.9.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.10.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.11.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.12.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.13.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.14.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.15.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.16.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.17.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.18.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.19.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.20.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.21.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.22.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.23.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.24.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.25.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.26.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.27.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.28.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.29.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.30.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.31.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.norm.weight', 'mistral_model.base_model.model.lm_head.weight', 'mistral_proj.weight', 'mistral_proj.bias'], unexpected_keys=[])\n",
      "Model Initialization Finished\n"
     ]
    }
   ],
   "source": [
    "# In[1]# In[1]:\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from math import ceil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import decord\n",
    "\n",
    "decord.bridge.set_bridge('torch')\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.easydict import EasyDict\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from decord import VideoReader, cpu\n",
    "import torchvision.transforms as T\n",
    "from dataset.video_transforms import (\n",
    "    GroupNormalize, GroupScale, GroupCenterCrop, \n",
    "    Stack, ToTorchFormatTensor\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from io import BytesIO\n",
    "from models import *\n",
    "\n",
    "try:\n",
    "    from petrel_client.client import Client\n",
    "    has_client = True\n",
    "    print(\"Client on!\")\n",
    "except:\n",
    "    has_client = False\n",
    "    print(\"Client off!\")\n",
    "\n",
    "if has_client:\n",
    "    client = Client('~/petreloss.conf')\n",
    "else:\n",
    "    client = None\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    #与测试任务无关\n",
    "    parser.add_argument('--model_type', default=\"VideoChat2_it4_mistral_LinearProAda\")\n",
    "    parser.add_argument('--model_dir', default=\"./download/parameters\")\n",
    "    parser.add_argument('--model_pth', default=\"timesuite\")\n",
    "    parser.add_argument('--output_dir', default=\"Please input model output dir!\")\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "    parser.add_argument('--infer_clip_frames', type=int, default=8)\n",
    "    \n",
    "    args = parser.parse_args(args=[])    \n",
    "    return args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "args_list_str = '\\n' + '\\n'.join([f'{k:<25}: {v}' for k, v in vars(args).items()])\n",
    "print(args_list_str)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "# config_file = \"configs/config_mistral.json\"\n",
    "config_file = args.model_dir+\"/config.json\"\n",
    "\n",
    "cfg = Config.from_file(config_file)\n",
    "cfg.model.use_lora = False\n",
    "cfg.model.pretrained_path=None\n",
    "cfg.device=\"cuda:7\"\n",
    "\n",
    "\n",
    "print(\"vision_encoder.num_frames:\", cfg.model.vision_encoder.num_frames)\n",
    "# cfg.model.vision_encoder.num_frames = 4\n",
    "\n",
    "model_cls = eval(args.model_type)\n",
    "# model = VideoChat2_it_mistral(config=cfg.model)\n",
    "model = model_cls(config=cfg.model)\n",
    "\n",
    "\n",
    "# add lora to run stage3 model\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, \n",
    "    r=16, lora_alpha=32, lora_dropout=0.,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "         \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "    ]\n",
    ")\n",
    "model.mistral_model = get_peft_model(model.mistral_model, peft_config)\n",
    "\n",
    "\n",
    "# state_dict = torch.load(\"./download/parameters/videochat2_mistral_7b_stage3.pth\", \"cpu\")\n",
    "state_dict = torch.load(args.model_dir+\"/\"+args.model_pth+\".pth\", \"cpu\")\n",
    "\n",
    "\n",
    "if 'model' in state_dict.keys():\n",
    "    msg = model.load_state_dict(state_dict['model'], strict=False)\n",
    "else:\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "print(msg)\n",
    "\n",
    "model = model.to(torch.device(cfg.device))\n",
    "model = model.eval()\n",
    "\n",
    "print('Model Initialization Finished')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    for role, message in conv.messages:\n",
    "        if message:\n",
    "            ret += role + \" \" + message + \" \" + conv.sep\n",
    "        else:\n",
    "            ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_prompt2(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    count = 0\n",
    "    for role, message in conv.messages:\n",
    "        count += 1\n",
    "        if count == len(conv.messages):\n",
    "            ret += role + \" \" + message\n",
    "        else:\n",
    "            if message:\n",
    "                ret += role + \" \" + message + \" \" + conv.sep\n",
    "            else:\n",
    "                ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_context_emb(conv, model, img_list, answer_prompt=None, print_res=False):\n",
    "    if answer_prompt:\n",
    "        prompt = get_prompt2(conv)\n",
    "    else:\n",
    "        prompt = get_prompt(conv)\n",
    "    if print_res:\n",
    "        print(\"prompt:\",prompt)\n",
    "    if '<VideoHere>' in prompt:\n",
    "        prompt_segs = prompt.split('<VideoHere>')\n",
    "    else:\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "    assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "    with torch.no_grad():\n",
    "        seg_tokens = [\n",
    "            model.mistral_tokenizer(\n",
    "                seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(cfg.device).input_ids\n",
    "            # only add bos to the first seg\n",
    "            for i, seg in enumerate(prompt_segs)\n",
    "        ]\n",
    "        seg_embs = [model.mistral_model.base_model.model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "#         seg_embs = [model.mistral_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "    mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "    mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "    return mixed_embs\n",
    "\n",
    "\n",
    "def ask(text, conv):\n",
    "    conv.messages.append([conv.roles[0], text])\n",
    "        \n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def answer(conv, model, img_list, do_sample=True, max_new_tokens=200, num_beams=1, min_length=1, top_p=0.9,\n",
    "               repetition_penalty=1.0, length_penalty=1, temperature=1.0, answer_prompt=None, print_res=False):\n",
    "    stop_words_ids = [\n",
    "        torch.tensor([2]).to(cfg.device),\n",
    "        torch.tensor([29871, 2]).to(cfg.device)]  # '</s>' can be encoded in two different ways.\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    conv.messages.append([conv.roles[1], answer_prompt])\n",
    "    embs = get_context_emb(conv, model, img_list, answer_prompt=answer_prompt, print_res=print_res)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.mistral_model.generate(\n",
    "            inputs_embeds=embs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=do_sample,\n",
    "            min_length=min_length,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    output_token = outputs[0]\n",
    "    if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    output_text = model.mistral_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "    output_text = output_text.split('</s>')[0]  # remove the stop sign </s>\n",
    "#     output_text = output_text.split('[/INST]')[-1].strip()\n",
    "    conv.messages[-1][1] = output_text + '</s>'\n",
    "    return output_text, output_token.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "def get_index(num_frames, num_segments):\n",
    "    seg_size = float(num_frames - 1) / num_segments\n",
    "    start = int(seg_size / 2)\n",
    "    offsets = np.array([\n",
    "        start + int(np.round(seg_size * idx)) for idx in range(num_segments)\n",
    "    ])\n",
    "    return offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(video_path, num_segments=8, return_msg=False, resolution=224):\n",
    "    \n",
    "    if client is not None and \"s3\" in video_path:\n",
    "        video_bytes = client.get(video_path)\n",
    "        assert(video_bytes is not None)\n",
    "        vr = VideoReader(BytesIO(video_bytes), ctx=cpu(0), num_threads=1)\n",
    "    else:\n",
    "        vr = VideoReader(uri=video_path, ctx=cpu(0), num_threads=1)\n",
    "    num_frames = len(vr)\n",
    "    frame_indices = get_index(num_frames, num_segments)\n",
    "\n",
    "    # transform\n",
    "    crop_size = resolution\n",
    "    scale_size = resolution\n",
    "    input_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "    input_std = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "    transform = T.Compose([\n",
    "        GroupScale(int(scale_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        GroupCenterCrop(crop_size),\n",
    "        Stack(),\n",
    "        ToTorchFormatTensor(),\n",
    "        GroupNormalize(input_mean, input_std) \n",
    "    ])\n",
    "\n",
    "    images_group = list()\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].numpy())\n",
    "        images_group.append(img)\n",
    "    torch_imgs = transform(images_group)\n",
    "    if return_msg:\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "        msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds. \"\n",
    "        # sec = [str(round(f / fps, 1)) for f in frame_indices]\n",
    "        # msg = f\"The video contains {len(frame_indices)} frames uniformly sampled from {sec[0]} to {sec[-1]} seconds. \"\n",
    "        return torch_imgs, msg\n",
    "    else:\n",
    "        return torch_imgs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_videochat(vid_path, user_messages):\n",
    "    \n",
    "    num_frame = model.clip_frames\n",
    "    tot_frames = model.total_frames\n",
    "    resolution = cfg.model.vision_encoder.img_size\n",
    "    \n",
    "    vid, msg = load_video(vid_path, num_segments=tot_frames, return_msg=True, resolution=resolution)\n",
    "\n",
    "    # The model expects inputs of shape: T x C x H x W\n",
    "    TC, H, W = vid.shape\n",
    "    video = vid.reshape(1, TC//3, 3, H, W).to(cfg.device)\n",
    "\n",
    "    img_list = []\n",
    "    with torch.no_grad():\n",
    "        image_emb = model.encode_long_video(video,[msg,],\"\")\n",
    "        print(\"Shape of long video embeds: \", image_emb.shape)\n",
    "#         image_emb, _ = model.encode_img(video, \"\")\n",
    "    img_list.append(image_emb)\n",
    "    \n",
    "    chat = EasyDict({\n",
    "    \"system\": \"You are able to understand the visual content that the user provides. Follow the instructions carefully and explain your answers in detail. \",\n",
    "    \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"\"\n",
    "    })\n",
    "    \n",
    "    chat.messages.append([chat.roles[0], \"<Video><VideoHere></Video> [/INST]\"])\n",
    "    ask(msg+user_messages, chat)\n",
    "\n",
    "    llm_answer = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "    print(\"LLM answer:\", llm_answer,\"\\n\\n\\n\")\n",
    "    \n",
    "    return llm_answer, chat, img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/zengxiangyu/.conda/envs/videochat/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/mnt/petrelfs/zengxiangyu/.conda/envs/videochat/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of long video embeds:  torch.Size([1, 384, 4096])\n",
      "prompt: You are able to understand the visual content that the user provides. Follow the instructions carefully and explain your answers in detail. [INST] <Video><VideoHere></Video> [/INST] [INST] The video contains 128 frames sampled at 0.4, 1.2, 2.0, 2.8, 3.7, 4.5, 5.3, 6.1, 6.9, 7.7, 8.5, 9.4, 10.2, 11.0, 11.8, 12.6, 13.4, 14.2, 15.1, 15.9, 16.7, 17.5, 18.3, 19.1, 19.9, 20.8, 21.6, 22.4, 23.2, 24.0, 24.8, 25.7, 26.5, 27.3, 28.1, 28.9, 29.7, 30.5, 31.4, 32.2, 33.0, 33.8, 34.6, 35.4, 36.2, 37.1, 37.9, 38.7, 39.5, 40.3, 41.1, 41.9, 42.8, 43.6, 44.4, 45.2, 46.0, 46.8, 47.6, 48.5, 49.3, 50.1, 50.9, 51.7, 52.5, 53.3, 54.2, 55.0, 55.8, 56.6, 57.4, 58.2, 59.1, 59.9, 60.7, 61.5, 62.3, 63.1, 63.9, 64.8, 65.6, 66.4, 67.2, 68.0, 68.8, 69.6, 70.5, 71.3, 72.1, 72.9, 73.7, 74.5, 75.3, 76.2, 77.0, 77.8, 78.6, 79.4, 80.2, 81.0, 81.9, 82.7, 83.5, 84.3, 85.1, 85.9, 86.7, 87.6, 88.4, 89.2, 90.0, 90.8, 91.6, 92.4, 93.3, 94.1, 94.9, 95.7, 96.5, 97.3, 98.1, 99.0, 99.8, 100.6, 101.4, 102.2, 103.0, 103.9 seconds. At what point does the man in the brown suit extend his hand to his deskmate? [/INST]\n",
      "LLM answer: 72.0 - 77.0 seconds.  \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('72.0 - 77.0 seconds. ',\n",
       " {'system': 'You are able to understand the visual content that the user provides. Follow the instructions carefully and explain your answers in detail. ',\n",
       "  'roles': ['[INST]', '[/INST]'],\n",
       "  'messages': [['[INST]', '<Video><VideoHere></Video> [/INST]'],\n",
       "   ['[INST]',\n",
       "    'The video contains 128 frames sampled at 0.4, 1.2, 2.0, 2.8, 3.7, 4.5, 5.3, 6.1, 6.9, 7.7, 8.5, 9.4, 10.2, 11.0, 11.8, 12.6, 13.4, 14.2, 15.1, 15.9, 16.7, 17.5, 18.3, 19.1, 19.9, 20.8, 21.6, 22.4, 23.2, 24.0, 24.8, 25.7, 26.5, 27.3, 28.1, 28.9, 29.7, 30.5, 31.4, 32.2, 33.0, 33.8, 34.6, 35.4, 36.2, 37.1, 37.9, 38.7, 39.5, 40.3, 41.1, 41.9, 42.8, 43.6, 44.4, 45.2, 46.0, 46.8, 47.6, 48.5, 49.3, 50.1, 50.9, 51.7, 52.5, 53.3, 54.2, 55.0, 55.8, 56.6, 57.4, 58.2, 59.1, 59.9, 60.7, 61.5, 62.3, 63.1, 63.9, 64.8, 65.6, 66.4, 67.2, 68.0, 68.8, 69.6, 70.5, 71.3, 72.1, 72.9, 73.7, 74.5, 75.3, 76.2, 77.0, 77.8, 78.6, 79.4, 80.2, 81.0, 81.9, 82.7, 83.5, 84.3, 85.1, 85.9, 86.7, 87.6, 88.4, 89.2, 90.0, 90.8, 91.6, 92.4, 93.3, 94.1, 94.9, 95.7, 96.5, 97.3, 98.1, 99.0, 99.8, 100.6, 101.4, 102.2, 103.0, 103.9 seconds. At what point does the man in the brown suit extend his hand to his deskmate?'],\n",
       "   ['[/INST]', '72.0 - 77.0 seconds. </s>']],\n",
       "  'sep': ''},\n",
       " [tensor([[[ 7.6294e-05,  8.1787e-02,  1.5747e-01,  ...,  4.3152e-02,\n",
       "             5.6091e-02,  3.8818e-02],\n",
       "           [-9.4986e-03,  1.0168e-01, -5.8777e-02,  ..., -1.1971e-02,\n",
       "            -3.0136e-03,  5.3040e-02],\n",
       "           [-2.8229e-03,  4.7638e-02,  4.7119e-02,  ...,  1.2213e-01,\n",
       "            -2.6672e-02,  1.0300e-02],\n",
       "           ...,\n",
       "           [-1.7322e-01, -1.1133e-01,  2.9248e-01,  ..., -2.2842e-02,\n",
       "             1.8604e-01, -4.0527e-02],\n",
       "           [-4.0063e-01, -2.0898e-01, -5.1453e-02,  ..., -2.3535e-01,\n",
       "             1.5063e-01, -1.5393e-01],\n",
       "           [-3.1982e-01, -5.0635e-01, -1.1279e-01,  ...,  2.0813e-02,\n",
       "             4.4653e-01, -4.7388e-01]]], device='cuda:7', dtype=torch.float16)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vid_path = \"./download/demo_video/ikun.mp4\"\n",
    "# question=\"At what time in the video does the man dribble the basketball?\"\n",
    "# question=\"What did the man do after throwing away the basketball?\"\n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/sora.mp4\"\n",
    "# question = \"When does the close-up of this woman's face appear?\"\n",
    "# LLM answer: From 37.8 to 59.9 seconds.\n",
    "# question = \"Describe this video in detail.\"\n",
    "# LLM answer: A woman is seen walking down a street in the rain while wearing sunglasses and a leather jacket. She continues walking down the street and looking off into the distance.  \n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/rocket.mp4\"\n",
    "# question = \"At what time in the video the rocket ignite and launch?\"\n",
    "# LLM answer: From 10.0 to 13.0 seconds.\n",
    "# question = \"Describe this video in detail.\"\n",
    "# LLM answer: The video shows a rocket launching from a building and then a space station docking with the international space station. The rocket is white and red with a red star on it. The space station is white and gray with solar panels. The video is in Chinese and there are no people in the video.  \n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/movie.mp4\"\n",
    "# question = \"When in the video do black men and white women hug each other?\"\n",
    "# LLM answer: 10.0 - 15.0 seconds\n",
    "# question = \"What are the two men in the video putting in the briefcase in the car?.\"\n",
    "# LLM answer: Money.  \n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/legendof1900.mp4\"\n",
    "# question = \"When in the video is the man in a white suit lighting a cigarette by the piano?\"\n",
    "# LLM answer: From 168.0 to 171.0 seconds. \n",
    "# question = \"Why are the strings in the video hot enough to light a cigarette?\"\n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/TheWanderingEarth2.mp4\"\n",
    "# question = \"At what moment in the picture did the person in the red life jacket and camouflage uniform open the isolation door?\"\n",
    "\n",
    "\n",
    "vid_path = \"./download/demo_video/mrbean.mp4\"\n",
    "question = \"At what point does the man in the brown suit extend his hand to his deskmate?\"\n",
    "\n",
    "\n",
    "generate_videochat(vid_path,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VideoChat",
   "language": "python",
   "name": "python3912"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
