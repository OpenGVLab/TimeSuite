{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]# In[1]:\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from math import ceil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import decord\n",
    "\n",
    "decord.bridge.set_bridge('torch')\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.easydict import EasyDict\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from decord import VideoReader, cpu\n",
    "import torchvision.transforms as T\n",
    "from dataset.video_transforms import (\n",
    "    GroupNormalize, GroupScale, GroupCenterCrop, \n",
    "    Stack, ToTorchFormatTensor\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from io import BytesIO\n",
    "from models import *\n",
    "\n",
    "try:\n",
    "    from petrel_client.client import Client\n",
    "    has_client = True\n",
    "    print(\"Client on!\")\n",
    "except:\n",
    "    has_client = False\n",
    "    print(\"Client off!\")\n",
    "\n",
    "if has_client:\n",
    "    client = Client('~/petreloss.conf')\n",
    "else:\n",
    "    client = None\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    #与测试任务无关\n",
    "    parser.add_argument('--model_type', default=\"VideoChat2_it4_mistral_LinearProAda\")\n",
    "    parser.add_argument('--model_dir', default=\"./download/parameters\")\n",
    "    parser.add_argument('--model_pth', default=\"timesuite\")\n",
    "    parser.add_argument('--output_dir', default=\"Please input model output dir!\")\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "    parser.add_argument('--infer_clip_frames', type=int, default=8)\n",
    "    \n",
    "    args = parser.parse_args(args=[])    \n",
    "    return args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "args_list_str = '\\n' + '\\n'.join([f'{k:<25}: {v}' for k, v in vars(args).items()])\n",
    "print(args_list_str)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "# config_file = \"configs/config_mistral.json\"\n",
    "config_file = args.model_dir+\"/config.json\"\n",
    "\n",
    "cfg = Config.from_file(config_file)\n",
    "cfg.model.use_lora = False\n",
    "cfg.model.pretrained_path=None\n",
    "cfg.device=\"cuda:7\"\n",
    "\n",
    "\n",
    "print(\"vision_encoder.num_frames:\", cfg.model.vision_encoder.num_frames)\n",
    "# cfg.model.vision_encoder.num_frames = 4\n",
    "\n",
    "model_cls = eval(args.model_type)\n",
    "# model = VideoChat2_it_mistral(config=cfg.model)\n",
    "model = model_cls(config=cfg.model)\n",
    "\n",
    "\n",
    "# add lora to run stage3 model\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, \n",
    "    r=16, lora_alpha=32, lora_dropout=0.,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "         \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "    ]\n",
    ")\n",
    "model.mistral_model = get_peft_model(model.mistral_model, peft_config)\n",
    "\n",
    "\n",
    "# state_dict = torch.load(\"./download/parameters/videochat2_mistral_7b_stage3.pth\", \"cpu\")\n",
    "state_dict = torch.load(args.model_dir+\"/\"+args.model_pth+\".pth\", \"cpu\")\n",
    "\n",
    "\n",
    "if 'model' in state_dict.keys():\n",
    "    msg = model.load_state_dict(state_dict['model'], strict=False)\n",
    "else:\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "print(msg)\n",
    "\n",
    "model = model.to(torch.device(cfg.device))\n",
    "model = model.eval()\n",
    "\n",
    "print('Model Initialization Finished')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    for role, message in conv.messages:\n",
    "        if message:\n",
    "            ret += role + \" \" + message + \" \" + conv.sep\n",
    "        else:\n",
    "            ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_prompt2(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    count = 0\n",
    "    for role, message in conv.messages:\n",
    "        count += 1\n",
    "        if count == len(conv.messages):\n",
    "            ret += role + \" \" + message\n",
    "        else:\n",
    "            if message:\n",
    "                ret += role + \" \" + message + \" \" + conv.sep\n",
    "            else:\n",
    "                ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_context_emb(conv, model, img_list, answer_prompt=None, print_res=False):\n",
    "    if answer_prompt:\n",
    "        prompt = get_prompt2(conv)\n",
    "    else:\n",
    "        prompt = get_prompt(conv)\n",
    "    if print_res:\n",
    "        print(\"prompt:\",prompt)\n",
    "    if '<VideoHere>' in prompt:\n",
    "        prompt_segs = prompt.split('<VideoHere>')\n",
    "    else:\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "    assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "    with torch.no_grad():\n",
    "        seg_tokens = [\n",
    "            model.mistral_tokenizer(\n",
    "                seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(cfg.device).input_ids\n",
    "            # only add bos to the first seg\n",
    "            for i, seg in enumerate(prompt_segs)\n",
    "        ]\n",
    "        seg_embs = [model.mistral_model.base_model.model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "#         seg_embs = [model.mistral_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "    mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "    mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "    return mixed_embs\n",
    "\n",
    "\n",
    "def ask(text, conv):\n",
    "    conv.messages.append([conv.roles[0], text])\n",
    "        \n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def answer(conv, model, img_list, do_sample=True, max_new_tokens=200, num_beams=1, min_length=1, top_p=0.9,\n",
    "               repetition_penalty=1.0, length_penalty=1, temperature=1.0, answer_prompt=None, print_res=False):\n",
    "    stop_words_ids = [\n",
    "        torch.tensor([2]).to(cfg.device),\n",
    "        torch.tensor([29871, 2]).to(cfg.device)]  # '</s>' can be encoded in two different ways.\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    conv.messages.append([conv.roles[1], answer_prompt])\n",
    "    embs = get_context_emb(conv, model, img_list, answer_prompt=answer_prompt, print_res=print_res)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.mistral_model.generate(\n",
    "            inputs_embeds=embs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=do_sample,\n",
    "            min_length=min_length,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    output_token = outputs[0]\n",
    "    if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    output_text = model.mistral_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "    output_text = output_text.split('</s>')[0]  # remove the stop sign </s>\n",
    "#     output_text = output_text.split('[/INST]')[-1].strip()\n",
    "    conv.messages[-1][1] = output_text + '</s>'\n",
    "    return output_text, output_token.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "def get_index(num_frames, num_segments):\n",
    "    seg_size = float(num_frames - 1) / num_segments\n",
    "    start = int(seg_size / 2)\n",
    "    offsets = np.array([\n",
    "        start + int(np.round(seg_size * idx)) for idx in range(num_segments)\n",
    "    ])\n",
    "    return offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(video_path, num_segments=8, return_msg=False, resolution=224):\n",
    "    \n",
    "    if client is not None and \"s3\" in video_path:\n",
    "        video_bytes = client.get(video_path)\n",
    "        assert(video_bytes is not None)\n",
    "        vr = VideoReader(BytesIO(video_bytes), ctx=cpu(0), num_threads=1)\n",
    "    else:\n",
    "        vr = VideoReader(uri=video_path, ctx=cpu(0), num_threads=1)\n",
    "    num_frames = len(vr)\n",
    "    frame_indices = get_index(num_frames, num_segments)\n",
    "\n",
    "    # transform\n",
    "    crop_size = resolution\n",
    "    scale_size = resolution\n",
    "    input_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "    input_std = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "    transform = T.Compose([\n",
    "        GroupScale(int(scale_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        GroupCenterCrop(crop_size),\n",
    "        Stack(),\n",
    "        ToTorchFormatTensor(),\n",
    "        GroupNormalize(input_mean, input_std) \n",
    "    ])\n",
    "\n",
    "    images_group = list()\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].numpy())\n",
    "        images_group.append(img)\n",
    "    torch_imgs = transform(images_group)\n",
    "    if return_msg:\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "        msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds. \"\n",
    "        # sec = [str(round(f / fps, 1)) for f in frame_indices]\n",
    "        # msg = f\"The video contains {len(frame_indices)} frames uniformly sampled from {sec[0]} to {sec[-1]} seconds. \"\n",
    "        return torch_imgs, msg\n",
    "    else:\n",
    "        return torch_imgs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_videochat(vid_path, user_messages):\n",
    "    \n",
    "    num_frame = model.clip_frames\n",
    "    tot_frames = model.total_frames\n",
    "    resolution = cfg.model.vision_encoder.img_size\n",
    "    \n",
    "    vid, msg = load_video(vid_path, num_segments=tot_frames, return_msg=True, resolution=resolution)\n",
    "\n",
    "    # The model expects inputs of shape: T x C x H x W\n",
    "    TC, H, W = vid.shape\n",
    "    video = vid.reshape(1, TC//3, 3, H, W).to(cfg.device)\n",
    "\n",
    "    img_list = []\n",
    "    with torch.no_grad():\n",
    "        image_emb = model.encode_long_video(video,[msg,],\"\")\n",
    "        print(\"Shape of long video embeds: \", image_emb.shape)\n",
    "#         image_emb, _ = model.encode_img(video, \"\")\n",
    "    img_list.append(image_emb)\n",
    "    \n",
    "    chat = EasyDict({\n",
    "    \"system\": \"You are able to understand the visual content that the user provides. Follow the instructions carefully and explain your answers in detail. \",\n",
    "    \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"\"\n",
    "    })\n",
    "    \n",
    "    chat.messages.append([chat.roles[0], \"<Video><VideoHere></Video> [/INST]\"])\n",
    "    ask(msg+user_messages, chat)\n",
    "\n",
    "    llm_answer = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "    print(\"LLM answer:\", llm_answer,\"\\n\\n\\n\")\n",
    "    \n",
    "    return llm_answer, chat, img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vid_path = \"./download/demo_video/ikun.mp4\"\n",
    "# question=\"At what time in the video does the man dribble the basketball?\"\n",
    "# question=\"What did the man do after throwing away the basketball?\"\n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/sora.mp4\"\n",
    "# question = \"When does the close-up of this woman's face appear?\"\n",
    "# LLM answer: From 37.8 to 59.9 seconds.\n",
    "# question = \"Describe this video in detail.\"\n",
    "# LLM answer: A woman is seen walking down a street in the rain while wearing sunglasses and a leather jacket. She continues walking down the street and looking off into the distance.  \n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/rocket.mp4\"\n",
    "# question = \"At what time in the video the rocket ignite and launch?\"\n",
    "# LLM answer: From 10.0 to 13.0 seconds.\n",
    "# question = \"Describe this video in detail.\"\n",
    "# LLM answer: The video shows a rocket launching from a building and then a space station docking with the international space station. The rocket is white and red with a red star on it. The space station is white and gray with solar panels. The video is in Chinese and there are no people in the video.  \n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/movie.mp4\"\n",
    "# question = \"When in the video do black men and white women hug each other?\"\n",
    "# LLM answer: 10.0 - 15.0 seconds\n",
    "# question = \"What are the two men in the video putting in the briefcase in the car?.\"\n",
    "# LLM answer: Money.  \n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/legendof1900.mp4\"\n",
    "# question = \"When in the video is the man in a white suit lighting a cigarette by the piano?\"\n",
    "# LLM answer: From 168.0 to 171.0 seconds. \n",
    "# question = \"Why are the strings in the video hot enough to light a cigarette?\"\n",
    "\n",
    "\n",
    "# vid_path = \"./download/demo_video/TheWanderingEarth2.mp4\"\n",
    "# question = \"At what moment in the picture did the person in the red life jacket and camouflage uniform open the isolation door?\"\n",
    "\n",
    "\n",
    "vid_path = \"./download/demo_video/mrbean.mp4\"\n",
    "question = \"At what point does the man in the brown suit extend his hand to his deskmate?\"\n",
    "\n",
    "\n",
    "generate_videochat(vid_path,question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VideoChat",
   "language": "python",
   "name": "python3912"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
