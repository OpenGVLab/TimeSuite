{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]# In[1]:\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from math import ceil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import decord\n",
    "\n",
    "decord.bridge.set_bridge('torch')\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.easydict import EasyDict\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from decord import VideoReader, cpu\n",
    "import torchvision.transforms as T\n",
    "from dataset.video_transforms import (\n",
    "    GroupNormalize, GroupScale, GroupCenterCrop, \n",
    "    Stack, ToTorchFormatTensor\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from io import BytesIO\n",
    "from models import *\n",
    "\n",
    "try:\n",
    "    from petrel_client.client import Client\n",
    "    has_client = True\n",
    "    print(\"Client on!\")\n",
    "except:\n",
    "    has_client = False\n",
    "    print(\"Client off!\")\n",
    "\n",
    "if has_client:\n",
    "    client = Client('~/petreloss.conf')\n",
    "else:\n",
    "    client = None\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    #与测试任务无关\n",
    "    parser.add_argument('--model_type', default=\"VideoChat2_it4_mistral_LinearProAda\")\n",
    "    parser.add_argument('--model_dir', default=\"./download/parameters\")\n",
    "    parser.add_argument('--model_pth', default=\"timesuite\")\n",
    "    parser.add_argument('--output_dir', default=\"Please input model output dir!\")\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "    parser.add_argument('--infer_clip_frames', type=int, default=8)\n",
    "    \n",
    "    args = parser.parse_args(args=[])    \n",
    "    return args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "args_list_str = '\\n' + '\\n'.join([f'{k:<25}: {v}' for k, v in vars(args).items()])\n",
    "print(args_list_str)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "# config_file = \"configs/config_mistral.json\"\n",
    "config_file = args.model_dir+\"/config.json\"\n",
    "\n",
    "cfg = Config.from_file(config_file)\n",
    "cfg.model.use_lora = False\n",
    "cfg.model.pretrained_path=None\n",
    "cfg.device=\"cuda\"\n",
    "\n",
    "print(\"vision_encoder.num_frames:\", cfg.model.vision_encoder.num_frames)\n",
    "# cfg.model.vision_encoder.num_frames = 4\n",
    "\n",
    "model_cls = eval(args.model_type)\n",
    "# model = VideoChat2_it_mistral(config=cfg.model)\n",
    "model = model_cls(config=cfg.model)\n",
    "\n",
    "\n",
    "# add lora to run stage3 model\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, \n",
    "    r=16, lora_alpha=32, lora_dropout=0.,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "         \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "    ]\n",
    ")\n",
    "model.mistral_model = get_peft_model(model.mistral_model, peft_config)\n",
    "\n",
    "\n",
    "state_dict = torch.load(args.model_dir+\"/\"+args.model_pth+\".pth\", \"cpu\")\n",
    "\n",
    "\n",
    "if 'model' in state_dict.keys():\n",
    "    msg = model.load_state_dict(state_dict['model'], strict=False)\n",
    "else:\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "print(msg)\n",
    "\n",
    "model = model.to(torch.device(cfg.device))\n",
    "model = model.eval()\n",
    "\n",
    "print('Model Initialization Finished')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    for role, message in conv.messages:\n",
    "        if message:\n",
    "            ret += role + \" \" + message + \" \" + conv.sep\n",
    "        else:\n",
    "            ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_prompt2(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    count = 0\n",
    "    for role, message in conv.messages:\n",
    "        count += 1\n",
    "        if count == len(conv.messages):\n",
    "            ret += role + \" \" + message\n",
    "        else:\n",
    "            if message:\n",
    "                ret += role + \" \" + message + \" \" + conv.sep\n",
    "            else:\n",
    "                ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_context_emb(conv, model, img_list, answer_prompt=None, print_res=False):\n",
    "    if answer_prompt:\n",
    "        prompt = get_prompt2(conv)\n",
    "    else:\n",
    "        prompt = get_prompt(conv)\n",
    "    if print_res:\n",
    "        print(\"prompt:\",prompt)\n",
    "    if '<VideoHere>' in prompt:\n",
    "        prompt_segs = prompt.split('<VideoHere>')\n",
    "    else:\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "    assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "    with torch.no_grad():\n",
    "        seg_tokens = [\n",
    "            model.mistral_tokenizer(\n",
    "                seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(cfg.device).input_ids\n",
    "            # only add bos to the first seg\n",
    "            for i, seg in enumerate(prompt_segs)\n",
    "        ]\n",
    "        seg_embs = [model.mistral_model.base_model.model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "#         seg_embs = [model.mistral_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "    mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "    mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "    return mixed_embs\n",
    "\n",
    "\n",
    "def ask(text, conv):\n",
    "    conv.messages.append([conv.roles[0], text])\n",
    "        \n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def answer(conv, model, img_list, do_sample=True, max_new_tokens=200, num_beams=1, min_length=1, top_p=0.9,\n",
    "               repetition_penalty=1.0, length_penalty=1, temperature=1.0, answer_prompt=None, print_res=False):\n",
    "    stop_words_ids = [\n",
    "        torch.tensor([2]).to(cfg.device),\n",
    "        torch.tensor([29871, 2]).to(cfg.device)]  # '</s>' can be encoded in two different ways.\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    conv.messages.append([conv.roles[1], answer_prompt])\n",
    "    embs = get_context_emb(conv, model, img_list, answer_prompt=answer_prompt, print_res=print_res)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.mistral_model.generate(\n",
    "            inputs_embeds=embs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=do_sample,\n",
    "            min_length=min_length,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    output_token = outputs[0]\n",
    "    if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    output_text = model.mistral_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "    output_text = output_text.split('</s>')[0]  # remove the stop sign </s>\n",
    "#     output_text = output_text.split('[/INST]')[-1].strip()\n",
    "    conv.messages[-1][1] = output_text + '</s>'\n",
    "    return output_text, output_token.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "def get_index(num_frames, num_segments):\n",
    "    seg_size = float(num_frames - 1) / num_segments\n",
    "    start = int(seg_size / 2)\n",
    "    offsets = np.array([\n",
    "        start + int(np.round(seg_size * idx)) for idx in range(num_segments)\n",
    "    ])\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def load_video(video_path, num_segments=8, return_msg=False, resolution=224):\n",
    "    \n",
    "    if client is not None and \"s3\" in video_path:\n",
    "        video_bytes = client.get(video_path)\n",
    "        assert(video_bytes is not None)\n",
    "        vr = VideoReader(BytesIO(video_bytes), ctx=cpu(0), num_threads=1)\n",
    "    else:\n",
    "        vr = VideoReader(uri=video_path, ctx=cpu(0), num_threads=1)\n",
    "    num_frames = len(vr)\n",
    "    frame_indices = get_index(num_frames, num_segments)\n",
    "\n",
    "    # transform\n",
    "    crop_size = resolution\n",
    "    scale_size = resolution\n",
    "    input_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "    input_std = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "    transform = T.Compose([\n",
    "        GroupScale(int(scale_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        GroupCenterCrop(crop_size),\n",
    "        Stack(),\n",
    "        ToTorchFormatTensor(),\n",
    "        GroupNormalize(input_mean, input_std) \n",
    "    ])\n",
    "\n",
    "    images_group = list()\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].numpy())\n",
    "        images_group.append(img)\n",
    "    torch_imgs = transform(images_group)\n",
    "    if return_msg:\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "        # \" \" should be added in the start and end\n",
    "        msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds. \"\n",
    "        return torch_imgs, msg\n",
    "    else:\n",
    "        return torch_imgs\n",
    "    \n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def generate_videochat(vid_path, user_messages):\n",
    "    \n",
    "    num_frame = model.clip_frames\n",
    "    tot_frames = model.total_frames\n",
    "    resolution = cfg.model.vision_encoder.img_size\n",
    "    \n",
    "    vid, msg = load_video(vid_path, num_segments=tot_frames, return_msg=True, resolution=resolution)\n",
    "\n",
    "    # The model expects inputs of shape: T x C x H x W\n",
    "    TC, H, W = vid.shape\n",
    "    video = vid.reshape(1, TC//3, 3, H, W).to(cfg.device)\n",
    "\n",
    "    img_list = []\n",
    "    with torch.no_grad():\n",
    "        image_emb = model.encode_long_video(video,[msg,],\"\")\n",
    "        print(\"Shape of long video embeds: \", image_emb.shape)\n",
    "#         image_emb, _ = model.encode_img(video, \"\")\n",
    "    img_list.append(image_emb)\n",
    "    \n",
    "    chat = EasyDict({\n",
    "    \"system\": \"You are able to understand the visual content that the user provides. Follow the instructions carefully and explain your answers in detail. \",\n",
    "    \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"\"\n",
    "    })\n",
    "    \n",
    "    chat.messages.append([chat.roles[0], \"<Video><VideoHere></Video> [/INST]\"])\n",
    "    ask(msg+user_messages, chat)\n",
    "\n",
    "    llm_answer = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "    print(\"LLM answer:\", llm_answer,\"\\n\\n\\n\")\n",
    "    \n",
    "    return llm_answer, chat, img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = \"./demo/example/yoga.mp4\"\n",
    "# vid_path = \"./example/jesse_dance.mp4\"\n",
    "\n",
    "question=\"Describe the video in details.\"\n",
    "\n",
    "generate_videochat(vid_path,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = {\n",
    "    \"Action Sequence\": (\"action_sequence.json\", \"pnorm2:s3://star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Action Prediction\": (\"action_prediction.json\", \"pnorm2:s3://star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Action Antonym\": (\"action_antonym.json\", \"pnorm2:s3://ssv2-video/\", \"video\", False),\n",
    "    \"Fine-grained Action\": (\"fine_grained_action.json\", \"pnorm:s3://Moments_in_Time_Raw/videos/\", \"video\", False),\n",
    "    \"Unexpected Action\": (\"unexpected_action.json\", \"pnorm2:s3://funqa-test/test/\", \"video\", False),\n",
    "    \"Object Existence\": (\"object_existence.json\", \"pnorm2:s3://clevrer/video_validation/\", \"video\", False),\n",
    "    \"Object Interaction\": (\"object_interaction.json\", \"pnorm2:s3://star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Object Shuffle\": (\"object_shuffle.json\", \"pnorm2:s3://perception/videos/\", \"video\", False),\n",
    "    \"Moving Direction\": (\"moving_direction.json\", \"pnorm2:s3://clevrer/video_validation/\", \"video\", False),\n",
    "    \"Action Localization\": (\"action_localization.json\", \"pnorm2:s3://sta/sta_video/\", \"video\", True),  # has start & end\n",
    "    \"Scene Transition\": (\"scene_transition.json\", \"pnorm2:s3://scene-qa/video/\", \"video\", False),\n",
    "    \"Action Count\": (\"action_count.json\", \"pnorm2:s3://perception/videos/\", \"video\", False),\n",
    "    \"Moving Count\": (\"moving_count.json\", \"pnorm2:s3://clevrer/video_validation/\", \"video\", False),\n",
    "    \"Moving Attribute\": (\"moving_attribute.json\", \"pnorm2:s3://clevrer/video_validation/\", \"video\", False),\n",
    "    \"State Change\": (\"state_change.json\", \"pnorm2:s3://perception/videos/\", \"video\", False),\n",
    "    \"Fine-grained Pose\": (\"fine_grained_pose.json\", \"pnorm2:s3://nturgbd/\", \"video\", False),\n",
    "    \"Character Order\": (\"character_order.json\", \"pnorm2:s3://perception/videos/\", \"video\", False),\n",
    "    \"Egocentric Navigation\": (\"egocentric_navigation.json\", \"pnorm2:s3://vlnqa/\", \"video\", False),\n",
    "    \"Episodic Reasoning\": (\"episodic_reasoning.json\", \"pnorm2:s3://tvqa/frames_fps3_hq/\", \"frame\", True),  # has start & end, read frame\n",
    "    \"Counterfactual Inference\": (\"counterfactual_inference.json\", \"pnorm2:s3://clevrer/video_validation/\", \"video\", False),\n",
    "}\n",
    "\n",
    "data_dir = \"./download/datasets/mvbench\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import io\n",
    "from io import BytesIO\n",
    "from petrel_client.client import Client\n",
    "from decord import VideoReader, cpu\n",
    "client = Client('~/petreloss.conf', enable_mc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frame = model.clip_frames\n",
    "tot_frames = model.total_frames\n",
    "resolution = cfg.model.vision_encoder.img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVBench_dataset(Dataset):\n",
    "    def __init__(self, data_dir, data_list, num_segments=8, resolution=224):\n",
    "        self.data_list = []\n",
    "        for k, v in data_list.items():\n",
    "            with open(os.path.join(data_dir, v[0]), 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "            for data in json_data:\n",
    "                self.data_list.append({\n",
    "                    'task_type': k,\n",
    "                    'prefix': v[1],\n",
    "                    'data_type': v[2],\n",
    "                    'bound': v[3],\n",
    "                    'data': data\n",
    "                })\n",
    "        \n",
    "        self.decord_method = {\n",
    "            'video': self.read_video,\n",
    "            'gif': self.read_gif,\n",
    "            'frame': self.read_frame,\n",
    "        }\n",
    "        \n",
    "        self.num_segments = num_segments\n",
    "        \n",
    "        # transform\n",
    "        crop_size = resolution\n",
    "        scale_size = resolution\n",
    "        input_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "        input_std = [0.26862954, 0.26130258, 0.27577711]\n",
    "        self.transform = T.Compose([\n",
    "            GroupScale(int(scale_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            GroupCenterCrop(crop_size),\n",
    "            Stack(),\n",
    "            ToTorchFormatTensor(),\n",
    "            GroupNormalize(input_mean, input_std) \n",
    "        ])\n",
    "    \n",
    "    def __str__(self):\n",
    "        len_list = {}\n",
    "        option_list = {}\n",
    "        for data in self.data_list:\n",
    "            if data['task_type'] not in len_list:\n",
    "                len_list[data['task_type']] = 0\n",
    "            len_list[data['task_type']] += 1\n",
    "            if data['task_type'] not in option_list:\n",
    "                option_list[data['task_type']] = 0\n",
    "            option_list[data['task_type']] += len(data['data']['candidates'])\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        res = f\"There are {len(self.data_list)} videos as follow:\\n\"\n",
    "        for k, v in len_list.items():\n",
    "            correct += len_list[k]\n",
    "            total += option_list[k]\n",
    "            res += f\"{v} for {k} ({option_list[k]} options => {len_list[k]/option_list[k]*100:.2f}%)\\n\"\n",
    "            correct = correct + 1 / option_list[k]\n",
    "        res += f\"Total random accuracy: {correct/total*100:.2f}%\"\n",
    "        return res.rstrip()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def get_index(self, bound, fps, max_frame, first_idx=0):\n",
    "        if bound:\n",
    "            start, end = bound[0], bound[1]\n",
    "        else:\n",
    "            start, end = -100000, 100000\n",
    "        start_idx = max(first_idx, round(start * fps))\n",
    "        end_idx = min(round(end * fps), max_frame)\n",
    "        seg_size = float(end_idx - start_idx) / self.num_segments\n",
    "        frame_indices = np.array([\n",
    "            int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "            for idx in range(self.num_segments)\n",
    "        ])\n",
    "        return frame_indices\n",
    "    \n",
    "    def read_video(self, video_path, bound=None, return_time=True):\n",
    "        if \"s3://\" in video_path:\n",
    "            video_bytes = client.get(video_path)\n",
    "            vr = VideoReader(io.BytesIO(video_bytes), ctx=cpu(0), num_threads=1)\n",
    "        else:\n",
    "            vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "        max_frame = len(vr) - 1\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        images_group = list()\n",
    "        frame_indices = self.get_index(bound, fps, max_frame, first_idx=0) \n",
    "        for frame_index in frame_indices:\n",
    "            img = Image.fromarray(vr[frame_index].numpy())\n",
    "            images_group.append(img)\n",
    "        torch_imgs = self.transform(images_group)\n",
    "        if return_time:\n",
    "            sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "            # \" \" should be added in the start and end\n",
    "            msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds. \"\n",
    "            return torch_imgs, msg\n",
    "        else:\n",
    "            return torch_imgs\n",
    "    \n",
    "    def read_gif(self, video_path, bound=None, fps=25, return_time=True):\n",
    "        if \"s3://\" in video_path:\n",
    "            video_bytes = client.get(video_path)\n",
    "            gif = imageio.get_reader(io.BytesIO(video_bytes))\n",
    "        else:\n",
    "            gif = imageio.get_reader(video_path)\n",
    "        max_frame = len(gif) - 1\n",
    "        \n",
    "        images_group = list()\n",
    "        frame_indices = self.get_index(bound, fps, max_frame, first_idx=0) \n",
    "        for index, frame in enumerate(gif):\n",
    "            if index in frame_indices:\n",
    "                img = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)\n",
    "                img = Image.fromarray(img)\n",
    "                images_group.append(img)\n",
    "        torch_imgs = self.transform(images_group)\n",
    "        if return_time:\n",
    "            sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "            # \" \" should be added in the start and end\n",
    "            msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds. \"\n",
    "            return torch_imgs, msg\n",
    "        else:\n",
    "            return torch_imgs\n",
    "    \n",
    "    def read_frame(self, video_path, bound=None, fps=3, return_time=True):\n",
    "        if os.path.exists(video_path):\n",
    "            max_frame = len(os.listdir(video_path))\n",
    "        else:\n",
    "            max_frame = len([k for k in client.list(video_path)])\n",
    "        images_group = list()\n",
    "        frame_indices = self.get_index(bound, fps, max_frame, first_idx=1) # frame_idx starts from 1\n",
    "        for frame_index in frame_indices:\n",
    "            if \"s3://\" in video_path:\n",
    "                img_bytes = client.get(os.path.join(video_path, f\"{frame_index:05d}.jpg\"))\n",
    "                img = Image.open(io.BytesIO(img_bytes))\n",
    "            else:\n",
    "                img = Image.open(os.path.join(video_path, f\"{frame_index:05d}.jpg\"))\n",
    "            images_group.append(img)\n",
    "        torch_imgs = self.transform(images_group)\n",
    "        if return_time:\n",
    "            sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "            # \" \" should be added in the start and end\n",
    "            msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds. \"\n",
    "            return torch_imgs, msg\n",
    "        else:\n",
    "            return torch_imgs\n",
    "\n",
    "    def qa_template(self, data):\n",
    "        question = f\"Question: {data['question']}\\n\"\n",
    "        question += \"Options:\\n\"\n",
    "        answer = data['answer']\n",
    "        answer_idx = -1\n",
    "        for idx, c in enumerate(data['candidates']):\n",
    "            question += f\"({chr(ord('A') + idx)}) {c}\\n\"\n",
    "            if c == answer:\n",
    "                answer_idx = idx\n",
    "        question = question.rstrip()\n",
    "        answer = f\"({chr(ord('A') + answer_idx)}) {answer}\"\n",
    "        return question, answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        decord_method = self.decord_method[self.data_list[idx]['data_type']]\n",
    "        bound = None\n",
    "        if self.data_list[idx]['bound']:\n",
    "            bound = (\n",
    "                self.data_list[idx]['data']['start'],\n",
    "                self.data_list[idx]['data']['end'],\n",
    "            )\n",
    "        video_path = os.path.join(self.data_list[idx]['prefix'], self.data_list[idx]['data']['video'])\n",
    "        torch_imgs, time_inst = decord_method(video_path, bound)\n",
    "        question, answer = self.qa_template(self.data_list[idx]['data'])\n",
    "            \n",
    "        return {\n",
    "            'video': torch_imgs, \n",
    "            'question': question, \n",
    "            'answer': answer,\n",
    "            'task_type': self.data_list[idx]['task_type'],\n",
    "            'time': time_inst,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MVBench_dataset(data_dir, data_list, num_segments=tot_frames, resolution=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_mvbench(\n",
    "        data_sample, system=\"\", \n",
    "        question_prompt='', # add in the end of question\n",
    "        answer_prompt=None, # add in the begining of answer\n",
    "        return_prompt='',  # add in the begining of return message\n",
    "        system_q=False, # whether add question in the system prompt for QFormer\n",
    "        print_res=True,\n",
    "        system_llm=False\n",
    "    ):\n",
    "    video = data_sample[\"video\"]\n",
    "    msg=data_sample[\"time\"]\n",
    "    TC, H, W = video.shape\n",
    "    video = video.reshape(1, TC//3, 3, H, W).to(cfg.device)\n",
    "    video_list = []\n",
    "    with torch.no_grad():\n",
    "        video_emb = model.encode_long_video(video,[msg,],\"\")\n",
    "    video_list.append(video_emb)\n",
    "#     video_list.append(torch.zeros_like(video_emb))\n",
    "\n",
    "    chat = EasyDict({\n",
    "        \"system\": system,\n",
    "        \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "        \"messages\": [],\n",
    "        \"sep\": \"\"\n",
    "    })\n",
    "\n",
    "    chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> [/INST]\"])\n",
    "    \n",
    "    if system_llm:\n",
    "        prompt = msg + system + data_sample['question'] + question_prompt\n",
    "    else:\n",
    "        prompt = msg + data_sample['question'] + question_prompt\n",
    "    \n",
    "    ask(prompt, chat)\n",
    "\n",
    "    llm_message = answer(\n",
    "        conv=chat, model=model, do_sample=False, \n",
    "        img_list=video_list, max_new_tokens=100, \n",
    "        answer_prompt=answer_prompt, print_res=print_res\n",
    "    )[0]\n",
    "    # remove potential explanation\n",
    "    llm_message = return_prompt + llm_message.strip().split('\\n')[0]\n",
    "    print(llm_message)\n",
    "    print(f\"GT: {data_sample['answer']}\")\n",
    "    return llm_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ans(pred, gt):\n",
    "    flag = False\n",
    "    \n",
    "    pred_list = pred.lower().split(' ')\n",
    "    pred_option, pred_content = pred_list[0], ' '.join(pred_list[1:])\n",
    "    gt_list = gt.lower().split(' ')\n",
    "    gt_option, gt_content = gt_list[0], \" \".join(gt_list[1:])\n",
    "    if gt_content[-1] == '.':\n",
    "        gt_content = gt_content[:-1]\n",
    "    \n",
    "    if pred_option.replace('.', '') in gt_option:\n",
    "        flag = True\n",
    "    elif gt_option in pred_option:\n",
    "        flag = True\n",
    "#     elif gt_content in pred_content:\n",
    "#         flag = True\n",
    "#     elif gt_content.replace(\"a \", \"\") in pred_content:\n",
    "#         flag = True\n",
    "#     elif gt_content.replace(\"an \", \"\") in pred_content:\n",
    "#         flag = True\n",
    "        \n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "res_list = []\n",
    "acc_dict = {}\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    task_type = example['task_type']\n",
    "    if task_type not in acc_dict:\n",
    "        acc_dict[task_type] = [0, 0] # correct, total\n",
    "    acc_dict[task_type][1] += 1\n",
    "    total += 1\n",
    "    pred = infer_mvbench(\n",
    "        example, \n",
    "#         \"Carefully observe the video and choose the best option for the question. \", \n",
    "#         \"Carefully watch the video and pay attention to the cause, sequence of events, and object details and movements. Based on your observations, select the best option that accurately addresses the question. \",  # newPrompt\n",
    "#         \"Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question. \", # newPrompt2\n",
    "        \"Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.\\n\", # newPrompt2\n",
    "#         question_prompt=\"\\nOnly give the best option without any explanation.\",\n",
    "#         question_prompt=\"\\nThink it step by step. Only give the best option without any explanation.\", # prompt2\n",
    "        question_prompt=\"\\nOnly give the best option.\",  # prompt3\n",
    "        answer_prompt=\"Best option:(\",\n",
    "        return_prompt='(',\n",
    "        system_q=False,\n",
    "        print_res=True,\n",
    "        system_llm=True,\n",
    "    )\n",
    "    gt = example['answer']\n",
    "    res_list.append({\n",
    "        'pred': pred,\n",
    "        'gt': gt\n",
    "    })\n",
    "    if check_ans(pred=pred, gt=gt):\n",
    "        acc_dict[task_type][0] += 1\n",
    "        correct += 1\n",
    "    print(f\"Part  Acc: {acc_dict[task_type][0] / acc_dict[task_type][1] * 100 :.2f}%\")\n",
    "    print(f\"Total Acc: {correct / total * 100 :.2f}%\")\n",
    "    print('-' * 50, task_type, '-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_path = args.model_dir+\"/MVBench_test_\"+args.model_pth+\"/result\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(f\"{save_path}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"acc_dict\": acc_dict,\n",
    "        \"res_list\": res_list\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = dict()\n",
    "correct = 0\n",
    "total = 0\n",
    "for k, v in acc_dict.items():\n",
    "    final_res[k] = v[0] / v[1] * 100\n",
    "    correct += v[0]\n",
    "    total += v[1]    \n",
    "final_res['Avg'] = correct / total * 100\n",
    "\n",
    "print(final_res)\n",
    "\n",
    "# with open(\"upload_leaderboard.json\", \"w\") as f:\n",
    "#     json.dump(final_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_path = args.model_dir+\"/MVBench_test_\"+args.model_pth+\"/acc\"\n",
    "out = \"AS\tAP\tAA\tFA\tUA\tOE\tOI\tOS\tMD\tAL\tST\tAC\tMC\tMA\tSC\tFP\tCO\tEN\tER\tCI\tAvg\"\n",
    "out1 = \"AS\t\tAP\t\tAA\t\tFA\t\tUA\t\tOE\t\tOI\t\tOS\t\tMD\t\tAL\t\tST\t\tAC\t\tMC\t\tMA\t\tSC\t\tFP\t\tCO\t\tEN\t\tER\t\tCI\t\tAvg\"\n",
    "out2 = \"\"\n",
    "correct = 0\n",
    "total = 0\n",
    "with open(f\"{save_path}.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "    for k, v in json_data[\"acc_dict\"].items():\n",
    "        correct += v[0]\n",
    "        total += v[1]    \n",
    "        out2 += f\"{v[0]/v[1]*100:.2f}\\t\"\n",
    "out2 += f\"{correct/total*100:.2f}\"\n",
    "print(out)\n",
    "print(out2)\n",
    "\n",
    "with open(f\"{acc_path}.txt\", \"w\") as f:\n",
    "    f.write(out1+\"\\n\")\n",
    "    f.write(out2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_egoschema(pred, qid):\n",
    "    correct = 0\n",
    "    answer_content = ans_dict[qid]['content'].lower()\n",
    "    if answer_content[-1] == \".\":\n",
    "        answer_content = answer_content[:-1]\n",
    "    if ans_dict[qid]['answer'].lower() in pred.lower():\n",
    "        flag = True\n",
    "        for kk in [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\"]:\n",
    "            if kk != ans_dict[qid]['answer'].lower() and kk in pred.lower():\n",
    "                flag = ans_dict\n",
    "                break\n",
    "        if flag:\n",
    "            correct += 1\n",
    "    elif answer_content in pred.lower():\n",
    "        correct = 1\n",
    "    elif answer_content.replace(\"a \", \"\") in pred.lower():\n",
    "        correct = 1\n",
    "    elif answer_content.replace(\"an \", \"\") in pred.lower():\n",
    "        correct = 1\n",
    "    return correct\n",
    "\n",
    "def infer_egoschema(\n",
    "        data_sample, system=\"\", \n",
    "        question_prompt='', # add in the end of question\n",
    "        answer_prompt=None, # add in the begining of answer\n",
    "        return_prompt='',  # add in the begining of return message\n",
    "        system_q=False, # whether add question in the system prompt for QFormer\n",
    "        print_res=True,\n",
    "        system_llm=False,\n",
    "        num_segments=8,\n",
    "    ):\n",
    "    vid_path = os.path.join(\"shdd:s3://egoschema/videos\", data_sample['video'])\n",
    "    print(vid_path)\n",
    "    video, msg = load_video(vid_path, num_segments=num_segments, return_msg=True)\n",
    "    TC, H, W = video.shape\n",
    "    video = video.reshape(1, TC//3, 3, H, W).to(cfg.device)\n",
    "    \n",
    "    video_list = []\n",
    "    with torch.no_grad():\n",
    "        video_emb = model.encode_long_video(video,[msg,],\"\")\n",
    "    video_list.append(video_emb)\n",
    "#     video_list.append(torch.zeros_like(video_emb))\n",
    "\n",
    "    chat = EasyDict({\n",
    "        \"system\": system,\n",
    "        \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "        \"messages\": [],\n",
    "        \"sep\": \"\"\n",
    "    })\n",
    "\n",
    "    chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> [/INST]\"])\n",
    "    \n",
    "    if system_llm:\n",
    "        prompt = msg + system + data_sample['QA'][0]['q'] + question_prompt\n",
    "    else:\n",
    "        prompt = msg + data_sample['QA'][0]['q'] + question_prompt\n",
    "    \n",
    "    ask(prompt, chat)\n",
    "\n",
    "    llm_message = answer(\n",
    "        conv=chat, model=model, do_sample=False, \n",
    "        img_list=video_list, max_new_tokens=100, \n",
    "        answer_prompt=answer_prompt, print_res=print_res\n",
    "    )[0]\n",
    "    # remove potential explanation\n",
    "    llm_message = return_prompt + llm_message.strip().split('\\n')[0]\n",
    "    print(llm_message)\n",
    "    print(f\"GT: {data_sample['QA'][0]['a']}\")\n",
    "    return llm_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"./download/datasets/egoschema/EgoSchema.csv\", mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    json_data = []\n",
    "    ans_dict = {}\n",
    "    \n",
    "    for idx, msg in enumerate(reader):\n",
    "        if idx == 0:\n",
    "            print(msg)\n",
    "            continue\n",
    "            \n",
    "        video = msg[1] + '.mp4'\n",
    "        input_str = f\"Question: {msg[3].capitalize()}\\nOptions:\\n\"\n",
    "    \n",
    "        target_index = -1\n",
    "        for i, candidate in enumerate(msg[5:]):\n",
    "            option = chr(ord('A') + i)\n",
    "            input_str += f\"({option}) {candidate}\\n\"\n",
    "            if candidate == msg[4]:\n",
    "                target_index = i\n",
    "            \n",
    "        assert target_index != -1\n",
    "        correct = chr(ord('A') + target_index)\n",
    "        \n",
    "        json_data.append({\n",
    "            'video': video,\n",
    "            \"QA\": [{\n",
    "                \"i\": \"\",\n",
    "                \"q\": input_str.strip(),\n",
    "                \"a\": f\"Answer: ({correct}) {msg[4]}\",\n",
    "            }]\n",
    "        })\n",
    "\n",
    "        ans_dict[idx - 1] = {\n",
    "            'video': video,\n",
    "            'answer': f\"({correct})\",\n",
    "            'content': msg[4],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  position embedding\n",
    "# num_frame = 16\n",
    "# resolution = 224\n",
    "# new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2*num_frame, cur_frame=num_frame)\n",
    "# model.vision_encoder.encoder.pos_embed = new_pos_emb\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "total_num = len(json_data)\n",
    "\n",
    "output = \"\"\n",
    "\n",
    "for idx, example in enumerate(tqdm(json_data)):\n",
    "    start = time.time()\n",
    "    llm_message = infer_egoschema(\n",
    "        example, \n",
    "        \"Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.\\n\", \n",
    "        question_prompt=\"\\nOnly give the best option.\", \n",
    "        answer_prompt=\"Best option:(\",\n",
    "        return_prompt='(',\n",
    "        system_q=False,\n",
    "        print_res=True,\n",
    "        system_llm=False,\n",
    "        num_segments=tot_frames\n",
    "    )\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    output += (example[\"video\"] + '\\n')\n",
    "    output += (llm_message + '\\n')\n",
    "    correct += check_answer_egoschema(llm_message, idx)\n",
    "    total += 1\n",
    "    print(\"Acc:\", correct / total)\n",
    "    print('-' * 20, f'{idx+1}/{total_num} done,', f'cost: {duration:.2f}s', '-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = args.model_dir+\"/Egoschema_test_\"+args.model_pth+\"/result_subset\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "with open(save_path + \".txt\", \"a\") as f:\n",
    "    f.writelines(output)\n",
    "    \n",
    "acc_path = args.model_dir+\"/Egoschema_test_\"+args.model_pth+\"/acc_subset\"\n",
    "with open(f\"{acc_path}.txt\", \"w\") as f:\n",
    "    f.write(\"Acc: \" + str(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./download/datasets/egoschema/questions.json\", \"r\") as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "full_egoschema = []\n",
    "for data in full_data:\n",
    "    video = data['q_uid'] + '.mp4'\n",
    "    input_str = f\"Question: {data['question'].capitalize()}\\nOptions:\\n\"\n",
    "\n",
    "    for i, candidate in enumerate(['option 0', 'option 1', 'option 2', 'option 3', 'option 4']):\n",
    "        option = chr(ord('A') + i)\n",
    "        input_str += f\"({option}) {data[candidate]}\\n\"\n",
    "    \n",
    "    full_egoschema.append({\n",
    "        'q_uid': data['q_uid'],\n",
    "        'video': video,\n",
    "        \"QA\": [{\n",
    "            \"i\": \"\",\n",
    "            \"q\": input_str.strip(),\n",
    "            \"a\": \"\",\n",
    "        }]\n",
    "    })\n",
    "\n",
    "\n",
    "def infer_full_egoschema(\n",
    "        data_sample, system=\"\", \n",
    "        question_prompt='', # add in the end of question\n",
    "        answer_prompt=None, # add in the begining of answer\n",
    "        return_prompt='',  # add in the begining of return message\n",
    "        system_q=False, # whether add question in the system prompt for QFormer\n",
    "        print_res=True,\n",
    "        system_llm=False,\n",
    "        num_segments=8,\n",
    "    ):\n",
    "    vid_path = os.path.join(\"shdd:s3://egoschema/videos\", data_sample['video'])\n",
    "    print(vid_path)\n",
    "    video, msg = load_video(vid_path, num_segments=num_segments, return_msg=True)\n",
    "    TC, H, W = video.shape\n",
    "    video = video.reshape(1, TC//3, 3, H, W).to(cfg.device)\n",
    "    \n",
    "    video_list = []\n",
    "    with torch.no_grad():\n",
    "        video_emb = model.encode_long_video(video,[msg,],\"\")\n",
    "    video_list.append(video_emb)\n",
    "\n",
    "    chat = EasyDict({\n",
    "        \"system\": system,\n",
    "        \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "        \"messages\": [],\n",
    "        \"sep\": \"\"\n",
    "    })\n",
    "\n",
    "    chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> [/INST]\"])\n",
    "    \n",
    "    if system_llm:\n",
    "        prompt = msg + system + data_sample['QA'][0]['q'] + question_prompt\n",
    "    else:\n",
    "        prompt = msg + data_sample['QA'][0]['q'] + question_prompt\n",
    "    \n",
    "    ask(prompt, chat)\n",
    "\n",
    "    llm_message = answer(\n",
    "        conv=chat, model=model, do_sample=False, \n",
    "        img_list=video_list, max_new_tokens=100, \n",
    "        answer_prompt=answer_prompt, print_res=print_res\n",
    "    )[0]\n",
    "    # remove potential explanation\n",
    "    llm_message = return_prompt + llm_message.strip().split('\\n')[0]\n",
    "    print(llm_message)\n",
    "    return llm_message\n",
    "\n",
    "\n",
    "#  position embedding\n",
    "# num_frame = 16\n",
    "# resolution = 224\n",
    "# new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2*num_frame, cur_frame=num_frame)\n",
    "# model.vision_encoder.encoder.pos_embed = new_pos_emb\n",
    "\n",
    "\n",
    "ans_dict = {}\n",
    "\n",
    "for idx, example in enumerate(tqdm(full_egoschema)):\n",
    "    start = time.time()\n",
    "    llm_message = infer_full_egoschema(\n",
    "        example, \n",
    "        \"Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.\\n\", \n",
    "        question_prompt=\"\\nOnly give the best option.\", \n",
    "        answer_prompt=\"Best option:(\",\n",
    "        return_prompt='(',\n",
    "        system_q=False,\n",
    "        print_res=True,\n",
    "        system_llm=False,\n",
    "        num_segments=tot_frames,\n",
    "    )\n",
    "\n",
    "    assert llm_message[0] == '(' and llm_message[2] == ')'\n",
    "    ans = ord(llm_message[1]) - ord('A')\n",
    "    assert ans in [0, 1, 2, 3, 4]\n",
    "    ans_dict[example['q_uid']] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = args.model_dir+\"/Egoschema_test_\"+args.model_pth+\"/result\"\n",
    "with open(save_path + \".json\", \"w\") as f:\n",
    "    json.dump(ans_dict, f)\n",
    "\n",
    "# Then you can run https://github.com/egoschema/EgoSchema/blob/main/validate.py to get the score\n",
    "# python3 validate.py --f ./your_prediction.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysubs2\n",
    "import re\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import PILToTensor\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]\\[\\]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def read_vtt_and_concatenate(file_path, tokenizer, max_len=4096):\n",
    "    subs = pysubs2.load(file_path, encoding=\"utf-8\")\n",
    "        \n",
    "    prev = \"\"\n",
    "    subtitles = []\n",
    "    for caption in subs:\n",
    "        # Split the caption text into individual lines\n",
    "        lines = caption.text.split('\\n')\n",
    "        for line in lines:\n",
    "            # Clean the text and check for repetition\n",
    "            line = clean_text(line)\n",
    "            if prev != line and line:\n",
    "                subtitles.append(line)\n",
    "                prev = line\n",
    "\n",
    "    # Join subtitles to check length\n",
    "    full_text = ' '.join(subtitles)\n",
    "    tokenized_ids = tokenizer(full_text, add_special_tokens=False).input_ids\n",
    "\n",
    "    # If the tokenized length is within the limit, return the full text\n",
    "    if len(tokenized_ids) <= max_len:\n",
    "        return full_text\n",
    "\n",
    "    # Otherwise, we need to trim the text to fit within the limit\n",
    "    # We will keep the first half and the last half\n",
    "    half_len = max_len // 2\n",
    "    start_text = ' '.join(subtitles[:half_len])\n",
    "    end_text = ' '.join(subtitles[-half_len:])\n",
    "    \n",
    "    # Re-tokenize to ensure the total length is within the limit\n",
    "    start_tokenized_ids = tokenizer(start_text, add_special_tokens=False).input_ids\n",
    "    end_tokenized_ids = tokenizer(end_text, add_special_tokens=False).input_ids\n",
    "\n",
    "    # Adjust the lengths to fit within the max_len\n",
    "    while len(start_tokenized_ids) + len(end_tokenized_ids) > max_len:\n",
    "        if len(start_tokenized_ids) > len(end_tokenized_ids):\n",
    "            start_tokenized_ids.pop()\n",
    "        else:\n",
    "            end_tokenized_ids.pop(0)\n",
    "    \n",
    "    # Combine the adjusted parts\n",
    "    adjusted_text = tokenizer.decode(start_tokenized_ids) + ' ... ' + tokenizer.decode(end_tokenized_ids)\n",
    "    \n",
    "    return adjusted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MME_dataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_prefix=\"shdd:s3://VideoMME_0629/processed_1fps\",\n",
    "        subtitle_prefix=\"./download/datasets/videomme/subtitle_0629\",\n",
    "        anno_path=\"./download/datasets/videomme/Video-MME_0629.json\",\n",
    "        frame_dict_path=\"./download/datasets/videomme/video_mme_1fps.json\",\n",
    "        num_segments=16, \n",
    "        stride=0, # if stride >= 1, will return all frames according to FPS (1/stride), else return partial frames\n",
    "        resolution=224, \n",
    "        max_subtitle_len=4096, # max_tokens for subtitle\n",
    "    ):\n",
    "        self.data_prefix = data_prefix\n",
    "        self.subtitle_prefix = subtitle_prefix\n",
    "        with open(anno_path, 'r') as f:\n",
    "            self.data_list = json.load(f)\n",
    "        with open(frame_dict_path, 'r') as f:\n",
    "            self.frame_dict = json.load(f)\n",
    "        \n",
    "        self.num_segments = num_segments\n",
    "        self.stride = stride\n",
    "        self.resolution = resolution\n",
    "        self.max_subtitle_len = max_subtitle_len\n",
    "\n",
    "        # transform\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Lambda(lambda x: x.float().div(255.0)),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    \n",
    "    def __str__(self):\n",
    "        task_dict = {}\n",
    "        total = 0\n",
    "        for data in self.data_list:\n",
    "            if data['duration_category'] not in ans_dict:\n",
    "                task_dict[data['duration_category']] = {}\n",
    "            for q in data['questions']:\n",
    "                if q['task_type'] not in ans_dict[data['duration_category']]:\n",
    "                    ans_dict[data['duration_category']][q['task_type']] = 0\n",
    "                ans_dict[data['duration_category']][q['task_type']] += 1\n",
    "                total += 1\n",
    "\n",
    "        res = f\"There are {len(self.data_list)} videos.\\n\"\n",
    "        res += f\"There are {total} QAs.\\n\"\n",
    "        for k, v in task_dict.items():\n",
    "            res += f\"------{k}------\\n\"\n",
    "            for kk, vv in task_dict.items():\n",
    "                res += f\"{kk}: {vv}\\n\"\n",
    "                \n",
    "        return res.rstrip()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def get_index(self, max_frame):\n",
    "        start_idx = 0\n",
    "        end_idx = max_frame - 1\n",
    "        seg_size = float(max_frame - start_idx) / self.num_segments\n",
    "        frame_indices = np.array([\n",
    "            max(int(start_idx + (seg_size / 2) + np.round(seg_size * idx)), end_idx)\n",
    "            for idx in range(self.num_segments)\n",
    "        ])\n",
    "        return frame_indices\n",
    "\n",
    "    def get_time_stamp(self, video_path):\n",
    "        timestamp = video_path.split(\"_\")[-1].split(\".jpg\")[0]\n",
    "        minutes, seconds = map(int, timestamp.split(\":\"))\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        return total_seconds\n",
    "\n",
    "    def read_frame(self, video_name):\n",
    "        full_frame_list = []\n",
    "        for p in self.frame_dict[video_name]:\n",
    "            full_frame_list.append(os.path.join(self.data_prefix, video_name, 'frames', p))\n",
    "            \n",
    "        images_group = list()\n",
    "        time_list = []\n",
    "        if self.stride >= 1 and (len(full_frame_list) / self.stride) > self.num_segments:\n",
    "            frame_list = full_frame_list[::self.stride]\n",
    "        else:\n",
    "            # if len(full_frame_list) < self.num_segments: # return all frames if not, seem to be a little lower\n",
    "            #     frame_list = full_frame_list\n",
    "            # else:\n",
    "            frame_indices = get_index(len(full_frame_list), self.num_segments)\n",
    "            frame_list = [full_frame_list[idx] for idx in frame_indices]\n",
    "        # print(frame_list)\n",
    "        \n",
    "        for frame_path in frame_list:\n",
    "            time_stamp = self.get_time_stamp(frame_path)\n",
    "            time_list.append(time_stamp)\n",
    "            if \"s3://\" in frame_path:\n",
    "                img_bytes = client.get(frame_path)\n",
    "                img = Image.open(io.BytesIO(img_bytes))\n",
    "            else:\n",
    "                img = Image.open(frame_path)\n",
    "            img = img.resize((resolution, resolution))\n",
    "            img = PILToTensor()(img).unsqueeze(0)\n",
    "            images_group.append(img)\n",
    "        torch_imgs = self.transform(torch.vstack(images_group))\n",
    "        sec = \", \".join(map(str, time_list))\n",
    "        time_instruction = f\"The video contains {len(time_list)} frames sampled at {sec} seconds. \"\n",
    "        print(torch_imgs.shape)\n",
    "        return torch_imgs, time_instruction\n",
    "\n",
    "    def qa_template(self, data):\n",
    "        question = f\"Question: {data['question']}\\n\"\n",
    "        question += \"Options:\\n\"\n",
    "        answer = data['answer']\n",
    "        answer = f\"({answer}) {data['options'][ord(answer) - ord('A')][3:]}\"\n",
    "        for idx, c in enumerate(data['options']):\n",
    "            cur_choice, cur_text = c[0], c[3:]\n",
    "            question += f\"({cur_choice}) {cur_text}\\n\"\n",
    "        question = question.rstrip()\n",
    "        return question, answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name = self.data_list[idx]['videoID']\n",
    "        torch_imgs, time_instruction = self.read_frame(video_name)\n",
    "        duration_category = self.data_list[idx]['duration']\n",
    "        qa_list = []\n",
    "        for qa in self.data_list[idx]['questions']:\n",
    "            qa_list.append(self.qa_template(qa))\n",
    "\n",
    "        subtitle = \"\"\n",
    "        try:\n",
    "            subtitle_path = os.path.join(self.subtitle_prefix, video_name + \".srt\")\n",
    "            if os.path.exists(subtitle_path):\n",
    "                subtitle = read_vtt_and_concatenate(subtitle_path, model.mistral_tokenizer, self.max_subtitle_len)\n",
    "        except Exception:\n",
    "            subtitle = \"\"\n",
    "            print(f\"Error for {subtitle_path}\")\n",
    "            \n",
    "        return {\n",
    "            'subtitle': subtitle,\n",
    "            'video': torch_imgs, \n",
    "            'time_instruction': time_instruction,\n",
    "            'qa_list': qa_list,\n",
    "            'duration_category': duration_category\n",
    "        }\n",
    "\n",
    "    \n",
    "def infer_mme(\n",
    "        data_sample, system=\"\", \n",
    "        question_prompt='', # add in the end of question\n",
    "        answer_prompt=None, # add in the begining of answer\n",
    "        return_prompt='',  # add in the begining of return message\n",
    "        system_q=False, # whether add question in the system prompt for QFormer\n",
    "        print_res=True,\n",
    "        system_llm=False,\n",
    "        no_qformer_instruction=False,\n",
    "        qformer_instruction=None,\n",
    "        add_subtitle=False,\n",
    "    ):\n",
    "    assert system_q == False, \"do not support system_q now\"\n",
    "    video = data_sample[\"video\"]\n",
    "    msg = data_sample[\"time_instruction\"]\n",
    "    T_, C, H, W = video.shape\n",
    "    video = video.reshape(1, T_, C, H, W).to(cfg.device)\n",
    "    \n",
    "    video_list = []\n",
    "    with torch.no_grad():\n",
    "        video_emb = model.encode_long_video(video,[msg,],\"\")\n",
    "    video_list.append(video_emb[0].unsqueeze(0))\n",
    "    print(video_list[0].shape)\n",
    "\n",
    "    pred_list = []\n",
    "    gt_list = []\n",
    "    for idx, qa in enumerate(data_sample['qa_list']):\n",
    "        print(f\"----------qa_{idx}---------\", flush=True)\n",
    "        chat = EasyDict({\n",
    "            \"system\": system,\n",
    "            \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "            \"messages\": [],\n",
    "            \"sep\": \"\"\n",
    "        })\n",
    "        \n",
    "        if add_subtitle and data_sample['subtitle'] != '':\n",
    "            subtitle = f\"This video's subtitles are listed below: {data_sample['subtitle']}\"\n",
    "            chat.messages.append([chat.roles[0], f\"{subtitle}\\n<Video><VideoHere></Video> [/INST]\"])\n",
    "        else:\n",
    "            chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> [/INST]\"])\n",
    "        \n",
    "        if system_llm:\n",
    "            prompt = msg + system + qa[0] + question_prompt\n",
    "        else:\n",
    "            prompt = msg + qa[0] + question_prompt\n",
    "        \n",
    "        ask(prompt, chat)\n",
    "    \n",
    "        llm_message = answer(\n",
    "            conv=chat, model=model, do_sample=False, \n",
    "            img_list=video_list, max_new_tokens=256, \n",
    "            answer_prompt=answer_prompt, print_res=print_res\n",
    "        )[0]\n",
    "        # remove potential explanation\n",
    "        llm_message = return_prompt + llm_message.strip().split('\\n')[0]\n",
    "        print(f\"Pred: {llm_message}\", flush=True)\n",
    "        print(f\"GT: {qa[1]}\", flush=True)\n",
    "        pred_list.append(llm_message[1])\n",
    "        gt_list.append(qa[1][1])\n",
    "    return pred_list, gt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 0\n",
    "max_subtitle_len=8192\n",
    "data_prefix = \"shdd:s3://VideoMME_0629/processed_1fps\"\n",
    "anno_path = \"./download/datasets/videomme/Video-MME_0629.json\"\n",
    "frame_dict_path = \"./download/datasets/videomme/video_mme_1fps.json\"\n",
    "dataset = MME_dataset(\n",
    "    data_prefix=data_prefix, \n",
    "    anno_path=anno_path, \n",
    "    frame_dict_path=frame_dict_path,\n",
    "    num_segments=tot_frames, \n",
    "    stride=stride,\n",
    "    resolution=resolution,\n",
    "    max_subtitle_len=max_subtitle_len,\n",
    ")\n",
    "\n",
    "with open(anno_path, 'r') as f:\n",
    "    res_json_data = json.load(f)\n",
    "    \n",
    "\n",
    "    \n",
    "# Only Vision Information\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "res_list = []\n",
    "acc_dict = {}\n",
    "\n",
    "for idx, example in enumerate(tqdm(dataset)):\n",
    "    duration_category = example['duration_category']\n",
    "    if duration_category not in acc_dict:\n",
    "        acc_dict[duration_category] = [0, 0] # correct, total\n",
    "    qa_count = len(example['qa_list'])\n",
    "    acc_dict[duration_category][1] += qa_count\n",
    "    total += qa_count\n",
    "\n",
    "    \n",
    "    pred_list, gt_list = infer_mme(\n",
    "        example, \n",
    "        \"Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.\\n\", # newPrompt2\n",
    "        question_prompt=\"\\nOnly give the best option.\",  # prompt3\n",
    "        answer_prompt=\"Best option:(\",\n",
    "        return_prompt='(',\n",
    "        system_q=False,\n",
    "        print_res=True,\n",
    "        system_llm=True,\n",
    "        add_subtitle=False,\n",
    "    )\n",
    "    \n",
    "    res_list.append({\n",
    "        'pred': pred_list,\n",
    "        'gt': gt_list\n",
    "    })\n",
    "    qa_idx = 0\n",
    "    for pred, gt in zip(pred_list, gt_list):\n",
    "        if pred == gt:\n",
    "            acc_dict[duration_category][0] += 1\n",
    "            correct += 1\n",
    "        res_json_data[idx]['questions'][qa_idx]['response'] = pred\n",
    "        qa_idx += 1\n",
    "    print(f\"Part  Acc: {acc_dict[duration_category][0] / acc_dict[duration_category][1] * 100 :.2f}%\")\n",
    "    print(f\"Total Acc: {correct / total * 100 :.2f}%\")\n",
    "    print('-' * 50, duration_category, '-' * 50)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "save_path = args.model_dir+\"/VideoMME_test_\"+args.model_pth+\"/Wo_result\"\n",
    "acc_path = args.model_dir+\"/VideoMME_test_\"+args.model_pth+\"/Wo_acc\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "category_list = [\"short\", \"medium\", \"long\"]\n",
    "\n",
    "with open(f\"{save_path}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"acc_dict\": acc_dict,\n",
    "        \"res_list\": res_list\n",
    "    }, f)\n",
    "\n",
    "with open(f\"{save_path}_full.json\", \"w\") as f:\n",
    "    json.dump(res_json_data, f)\n",
    "\n",
    "with open(f\"{acc_path}.txt\", \"w\") as f:\n",
    "    f.write(\"Acc: \" + str(round(100 * correct / total, 1)) + \"\\n\")\n",
    "    for duration_category in category_list:\n",
    "        f.write(duration_category + \" Acc: \" + str(round(100 * acc_dict[duration_category][0] / acc_dict[duration_category][1], 1)) + \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "# With Subtitle\n",
    "correct = 0\n",
    "total = 0\n",
    "res_list = []\n",
    "acc_dict = {}\n",
    "\n",
    "for idx, example in enumerate(tqdm(dataset)):\n",
    "    duration_category = example['duration_category']\n",
    "    if duration_category not in acc_dict:\n",
    "        acc_dict[duration_category] = [0, 0] # correct, total\n",
    "    qa_count = len(example['qa_list'])\n",
    "    acc_dict[duration_category][1] += qa_count\n",
    "    total += qa_count\n",
    "\n",
    "    \n",
    "    pred_list, gt_list = infer_mme(\n",
    "        example, \n",
    "        \"Carefully watch the video, read the related subtitles and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.\\n\", # newPrompt2\n",
    "        question_prompt=\"\\nOnly give the best option.\",  # prompt3\n",
    "        answer_prompt=\"Best option:(\",\n",
    "        return_prompt='(',\n",
    "        system_q=False,\n",
    "        print_res=True,\n",
    "        system_llm=True,\n",
    "        add_subtitle=True,\n",
    "    )\n",
    "    \n",
    "    res_list.append({\n",
    "        'pred': pred_list,\n",
    "        'gt': gt_list\n",
    "    })\n",
    "    qa_idx = 0\n",
    "    for pred, gt in zip(pred_list, gt_list):\n",
    "        if pred == gt:\n",
    "            acc_dict[duration_category][0] += 1\n",
    "            correct += 1\n",
    "        res_json_data[idx]['questions'][qa_idx]['response'] = pred\n",
    "        qa_idx += 1\n",
    "    print(f\"Part  Acc: {acc_dict[duration_category][0] / acc_dict[duration_category][1] * 100 :.2f}%\")\n",
    "    print(f\"Total Acc: {correct / total * 100 :.2f}%\")\n",
    "    print('-' * 50, duration_category, '-' * 50)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "save_path = args.model_dir+\"/VideoMME_test_\"+args.model_pth+\"/WithSub_result\"\n",
    "acc_path = args.model_dir+\"/VideoMME_test_\"+args.model_pth+\"/WithSub_acc\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "category_list = [\"short\", \"medium\", \"long\"]\n",
    "\n",
    "with open(f\"{save_path}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"acc_dict\": acc_dict,\n",
    "        \"res_list\": res_list\n",
    "    }, f)\n",
    "\n",
    "with open(f\"{save_path}_full.json\", \"w\") as f:\n",
    "    json.dump(res_json_data, f)\n",
    "\n",
    "with open(f\"{acc_path}.txt\", \"w\") as f:\n",
    "    f.write(\"Acc: \" + str(round(100 * correct / total, 1)) + \"\\n\")\n",
    "    for duration_category in category_list:\n",
    "        f.write(duration_category + \" Acc: \" + str(round(100 * acc_dict[duration_category][0] / acc_dict[duration_category][1], 1)) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VideoChat",
   "language": "python",
   "name": "python3912"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
